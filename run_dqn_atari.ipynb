{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import dqn\n",
    "from dqn_utils import *\n",
    "from atari_wrappers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#a = -2 % 5\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 8\n"
     ]
    }
   ],
   "source": [
    "#a = [3, 8]\n",
    "#print(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def atari_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=512,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out\n",
    "\n",
    "def atari_learn(env,\n",
    "                session,\n",
    "                num_timesteps):\n",
    "    # This is just a rough estimate\n",
    "    num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "    lr_multiplier = 1.0\n",
    "    lr_schedule = PiecewiseSchedule([\n",
    "                                         (0,                   1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                    ],\n",
    "                                    outside_value=5e-5 * lr_multiplier)\n",
    "    optimizer = dqn.OptimizerSpec(\n",
    "        constructor=tf.train.AdamOptimizer,\n",
    "        kwargs=dict(epsilon=1e-4),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "\n",
    "    def stopping_criterion(env, t):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
    "\n",
    "    exploration_schedule = PiecewiseSchedule(\n",
    "        [\n",
    "            (0, 1.0),\n",
    "            (1e6, 0.1),\n",
    "            (num_iterations / 2, 0.01),\n",
    "        ], outside_value=0.01\n",
    "    )\n",
    "\n",
    "    dqn.learn(\n",
    "        env,\n",
    "        q_func=atari_model,\n",
    "        optimizer_spec=optimizer,\n",
    "        session=session,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=1000000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        learning_starts=50000,\n",
    "        learning_freq=4,\n",
    "        frame_history_len=4,\n",
    "        target_update_freq=10000,\n",
    "        grad_norm_clipping=10\n",
    "    )\n",
    "    env.close()\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def set_global_seeds(i):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        pass\n",
    "    else:\n",
    "        tf.set_random_seed(i) \n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "def get_session():\n",
    "    tf.reset_default_graph()\n",
    "    tf_config = tf.ConfigProto(\n",
    "        inter_op_parallelism_threads=1,\n",
    "        intra_op_parallelism_threads=1)\n",
    "    session = tf.Session(config=tf_config)\n",
    "    print(\"AVAILABLE GPUS: \", get_available_gpus())\n",
    "    return session\n",
    "\n",
    "def get_env(task, seed):\n",
    "    env_id = task.env_id\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    set_global_seeds(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    expt_dir = '/tmp/hw3_vid_dir2/'\n",
    "    env = wrappers.Monitor(env, osp.join(expt_dir, \"gym\"), force=True)\n",
    "    env = wrap_deepmind(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_Atari():\n",
    "    # Get Atari games.\n",
    "    benchmark = gym.benchmark_spec('Atari40M')\n",
    "\n",
    "    # Change the index to select a different game.\n",
    "    task = benchmark.tasks[3]\n",
    "\n",
    "    # Run training\n",
    "    seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "    env = get_env(task, seed)\n",
    "    session = get_session()\n",
    "    atari_learn(env, session, num_timesteps=task.max_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 14:02:06,080] Making new env: PongNoFrameskip-v4\n",
      "[2018-06-02 14:02:06,474] Clearing 14 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE GPUS:  ['device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 14:02:06,863] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000000.mp4\n",
      "[2018-06-02 14:02:10,064] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000001.mp4\n",
      "[2018-06-02 14:02:21,084] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000008.mp4\n",
      "[2018-06-02 14:02:44,206] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000027.mp4\n",
      "[2018-06-02 14:03:58,817] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 60000\n",
      "mean reward (100 episodes) -20.292308\n",
      "best mean reward -inf\n",
      "episodes 65\n",
      "exploration 0.946000\n",
      "learning_rate 0.000100\n",
      "Timestep 70000\n",
      "mean reward (100 episodes) -20.302632\n",
      "best mean reward -inf\n",
      "episodes 76\n",
      "exploration 0.937000\n",
      "learning_rate 0.000100\n",
      "Timestep 80000\n",
      "mean reward (100 episodes) -20.220930\n",
      "best mean reward -inf\n",
      "episodes 86\n",
      "exploration 0.928000\n",
      "learning_rate 0.000100\n",
      "Timestep 90000\n",
      "mean reward (100 episodes) -20.206186\n",
      "best mean reward -inf\n",
      "episodes 97\n",
      "exploration 0.919000\n",
      "learning_rate 0.000100\n",
      "Timestep 100000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.200000\n",
      "episodes 108\n",
      "exploration 0.910000\n",
      "learning_rate 0.000100\n",
      "Timestep 110000\n",
      "mean reward (100 episodes) -20.260000\n",
      "best mean reward -20.200000\n",
      "episodes 120\n",
      "exploration 0.901000\n",
      "learning_rate 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 14:08:22,368] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 120000\n",
      "mean reward (100 episodes) -20.240000\n",
      "best mean reward -20.200000\n",
      "episodes 130\n",
      "exploration 0.892000\n",
      "learning_rate 0.000100\n",
      "Timestep 130000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.160000\n",
      "episodes 140\n",
      "exploration 0.883000\n",
      "learning_rate 0.000100\n",
      "Timestep 140000\n",
      "mean reward (100 episodes) -20.250000\n",
      "best mean reward -20.160000\n",
      "episodes 152\n",
      "exploration 0.874000\n",
      "learning_rate 0.000100\n",
      "Timestep 150000\n",
      "mean reward (100 episodes) -20.240000\n",
      "best mean reward -20.160000\n",
      "episodes 163\n",
      "exploration 0.865000\n",
      "learning_rate 0.000100\n",
      "Timestep 160000\n",
      "mean reward (100 episodes) -20.170000\n",
      "best mean reward -20.160000\n",
      "episodes 173\n",
      "exploration 0.856000\n",
      "learning_rate 0.000100\n",
      "Timestep 170000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.160000\n",
      "episodes 184\n",
      "exploration 0.847000\n",
      "learning_rate 0.000100\n",
      "Timestep 180000\n",
      "mean reward (100 episodes) -20.320000\n",
      "best mean reward -20.160000\n",
      "episodes 195\n",
      "exploration 0.838000\n",
      "learning_rate 0.000100\n",
      "Timestep 190000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.160000\n",
      "episodes 205\n",
      "exploration 0.829000\n",
      "learning_rate 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 14:15:18,063] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 200000\n",
      "mean reward (100 episodes) -20.250000\n",
      "best mean reward -20.160000\n",
      "episodes 216\n",
      "exploration 0.820000\n",
      "learning_rate 0.000100\n",
      "Timestep 210000\n",
      "mean reward (100 episodes) -20.170000\n",
      "best mean reward -20.160000\n",
      "episodes 226\n",
      "exploration 0.811000\n",
      "learning_rate 0.000100\n",
      "Timestep 220000\n",
      "mean reward (100 episodes) -20.250000\n",
      "best mean reward -20.160000\n",
      "episodes 237\n",
      "exploration 0.802000\n",
      "learning_rate 0.000100\n",
      "Timestep 230000\n",
      "mean reward (100 episodes) -20.240000\n",
      "best mean reward -20.160000\n",
      "episodes 248\n",
      "exploration 0.793000\n",
      "learning_rate 0.000100\n",
      "Timestep 240000\n",
      "mean reward (100 episodes) -20.160000\n",
      "best mean reward -20.160000\n",
      "episodes 259\n",
      "exploration 0.784000\n",
      "learning_rate 0.000100\n",
      "Timestep 250000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.150000\n",
      "episodes 270\n",
      "exploration 0.775000\n",
      "learning_rate 0.000100\n",
      "Timestep 260000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.150000\n",
      "episodes 281\n",
      "exploration 0.766000\n",
      "learning_rate 0.000100\n",
      "Timestep 270000\n",
      "mean reward (100 episodes) -20.180000\n",
      "best mean reward -20.150000\n",
      "episodes 291\n",
      "exploration 0.757000\n",
      "learning_rate 0.000100\n",
      "Timestep 280000\n",
      "mean reward (100 episodes) -20.200000\n",
      "best mean reward -20.150000\n",
      "episodes 302\n",
      "exploration 0.748000\n",
      "learning_rate 0.000100\n",
      "Timestep 290000\n",
      "mean reward (100 episodes) -20.180000\n",
      "best mean reward -20.150000\n",
      "episodes 313\n",
      "exploration 0.739000\n",
      "learning_rate 0.000100\n",
      "Timestep 300000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.150000\n",
      "episodes 324\n",
      "exploration 0.730000\n",
      "learning_rate 0.000100\n",
      "Timestep 310000\n",
      "mean reward (100 episodes) -20.080000\n",
      "best mean reward -20.080000\n",
      "episodes 333\n",
      "exploration 0.721000\n",
      "learning_rate 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 14:24:29,461] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 320000\n",
      "mean reward (100 episodes) -20.090000\n",
      "best mean reward -20.040000\n",
      "episodes 343\n",
      "exploration 0.712000\n",
      "learning_rate 0.000100\n",
      "Timestep 330000\n",
      "mean reward (100 episodes) -20.040000\n",
      "best mean reward -20.020000\n",
      "episodes 353\n",
      "exploration 0.703000\n",
      "learning_rate 0.000100\n",
      "Timestep 340000\n",
      "mean reward (100 episodes) -19.940000\n",
      "best mean reward -19.940000\n",
      "episodes 363\n",
      "exploration 0.694000\n",
      "learning_rate 0.000100\n",
      "Timestep 350000\n",
      "mean reward (100 episodes) -19.890000\n",
      "best mean reward -19.870000\n",
      "episodes 373\n",
      "exploration 0.685000\n",
      "learning_rate 0.000100\n",
      "Timestep 360000\n",
      "mean reward (100 episodes) -19.880000\n",
      "best mean reward -19.870000\n",
      "episodes 384\n",
      "exploration 0.676000\n",
      "learning_rate 0.000100\n",
      "Timestep 370000\n",
      "mean reward (100 episodes) -19.870000\n",
      "best mean reward -19.860000\n",
      "episodes 394\n",
      "exploration 0.667000\n",
      "learning_rate 0.000100\n",
      "Timestep 380000\n",
      "mean reward (100 episodes) -19.890000\n",
      "best mean reward -19.840000\n",
      "episodes 404\n",
      "exploration 0.658000\n",
      "learning_rate 0.000100\n",
      "Timestep 390000\n",
      "mean reward (100 episodes) -19.810000\n",
      "best mean reward -19.810000\n",
      "episodes 414\n",
      "exploration 0.649000\n",
      "learning_rate 0.000100\n",
      "Timestep 400000\n",
      "mean reward (100 episodes) -19.760000\n",
      "best mean reward -19.760000\n",
      "episodes 424\n",
      "exploration 0.640000\n",
      "learning_rate 0.000100\n",
      "Timestep 410000\n",
      "mean reward (100 episodes) -19.880000\n",
      "best mean reward -19.760000\n",
      "episodes 434\n",
      "exploration 0.631000\n",
      "learning_rate 0.000100\n",
      "Timestep 420000\n",
      "mean reward (100 episodes) -19.940000\n",
      "best mean reward -19.760000\n",
      "episodes 444\n",
      "exploration 0.622000\n",
      "learning_rate 0.000100\n",
      "Timestep 430000\n",
      "mean reward (100 episodes) -19.960000\n",
      "best mean reward -19.760000\n",
      "episodes 454\n",
      "exploration 0.613000\n",
      "learning_rate 0.000100\n",
      "Timestep 440000\n",
      "mean reward (100 episodes) -20.050000\n",
      "best mean reward -19.760000\n",
      "episodes 463\n",
      "exploration 0.604000\n",
      "learning_rate 0.000100\n",
      "Timestep 450000\n",
      "mean reward (100 episodes) -20.100000\n",
      "best mean reward -19.760000\n",
      "episodes 473\n",
      "exploration 0.595000\n",
      "learning_rate 0.000100\n",
      "Timestep 460000\n",
      "mean reward (100 episodes) -20.100000\n",
      "best mean reward -19.760000\n",
      "episodes 482\n",
      "exploration 0.586000\n",
      "learning_rate 0.000100\n",
      "Timestep 470000\n",
      "mean reward (100 episodes) -19.900000\n",
      "best mean reward -19.760000\n",
      "episodes 490\n",
      "exploration 0.577000\n",
      "learning_rate 0.000100\n",
      "Timestep 480000\n",
      "mean reward (100 episodes) -19.900000\n",
      "best mean reward -19.760000\n",
      "episodes 498\n",
      "exploration 0.568000\n",
      "learning_rate 0.000100\n",
      "Timestep 490000\n",
      "mean reward (100 episodes) -19.800000\n",
      "best mean reward -19.760000\n",
      "episodes 506\n",
      "exploration 0.559000\n",
      "learning_rate 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 14:39:46,967] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 500000\n",
      "mean reward (100 episodes) -19.710000\n",
      "best mean reward -19.710000\n",
      "episodes 514\n",
      "exploration 0.550000\n",
      "learning_rate 0.000100\n",
      "Timestep 510000\n",
      "mean reward (100 episodes) -19.600000\n",
      "best mean reward -19.600000\n",
      "episodes 523\n",
      "exploration 0.541000\n",
      "learning_rate 0.000100\n",
      "Timestep 520000\n",
      "mean reward (100 episodes) -19.550000\n",
      "best mean reward -19.540000\n",
      "episodes 531\n",
      "exploration 0.532000\n",
      "learning_rate 0.000100\n",
      "Timestep 530000\n",
      "mean reward (100 episodes) -19.520000\n",
      "best mean reward -19.520000\n",
      "episodes 538\n",
      "exploration 0.523000\n",
      "learning_rate 0.000100\n",
      "Timestep 540000\n",
      "mean reward (100 episodes) -19.400000\n",
      "best mean reward -19.390000\n",
      "episodes 546\n",
      "exploration 0.514000\n",
      "learning_rate 0.000100\n",
      "Timestep 550000\n",
      "mean reward (100 episodes) -19.310000\n",
      "best mean reward -19.290000\n",
      "episodes 553\n",
      "exploration 0.505000\n",
      "learning_rate 0.000100\n",
      "Timestep 560000\n",
      "mean reward (100 episodes) -19.160000\n",
      "best mean reward -19.160000\n",
      "episodes 559\n",
      "exploration 0.496000\n",
      "learning_rate 0.000100\n",
      "Timestep 570000\n",
      "mean reward (100 episodes) -19.010000\n",
      "best mean reward -19.010000\n",
      "episodes 566\n",
      "exploration 0.487000\n",
      "learning_rate 0.000100\n",
      "Timestep 580000\n",
      "mean reward (100 episodes) -18.850000\n",
      "best mean reward -18.850000\n",
      "episodes 573\n",
      "exploration 0.478000\n",
      "learning_rate 0.000100\n",
      "Timestep 590000\n",
      "mean reward (100 episodes) -18.650000\n",
      "best mean reward -18.650000\n",
      "episodes 580\n",
      "exploration 0.469000\n",
      "learning_rate 0.000100\n",
      "Timestep 600000\n",
      "mean reward (100 episodes) -18.640000\n",
      "best mean reward -18.610000\n",
      "episodes 586\n",
      "exploration 0.460000\n",
      "learning_rate 0.000100\n",
      "Timestep 610000\n",
      "mean reward (100 episodes) -18.690000\n",
      "best mean reward -18.610000\n",
      "episodes 593\n",
      "exploration 0.451000\n",
      "learning_rate 0.000100\n",
      "Timestep 620000\n",
      "mean reward (100 episodes) -18.580000\n",
      "best mean reward -18.580000\n",
      "episodes 599\n",
      "exploration 0.442000\n",
      "learning_rate 0.000100\n",
      "Timestep 630000\n",
      "mean reward (100 episodes) -18.520000\n",
      "best mean reward -18.520000\n",
      "episodes 604\n",
      "exploration 0.433000\n",
      "learning_rate 0.000100\n",
      "Timestep 640000\n",
      "mean reward (100 episodes) -18.340000\n",
      "best mean reward -18.340000\n",
      "episodes 610\n",
      "exploration 0.424000\n",
      "learning_rate 0.000100\n",
      "Timestep 650000\n",
      "mean reward (100 episodes) -18.260000\n",
      "best mean reward -18.250000\n",
      "episodes 615\n",
      "exploration 0.415000\n",
      "learning_rate 0.000100\n",
      "Timestep 660000\n",
      "mean reward (100 episodes) -18.200000\n",
      "best mean reward -18.200000\n",
      "episodes 620\n",
      "exploration 0.406000\n",
      "learning_rate 0.000100\n",
      "Timestep 670000\n",
      "mean reward (100 episodes) -18.000000\n",
      "best mean reward -18.000000\n",
      "episodes 626\n",
      "exploration 0.397000\n",
      "learning_rate 0.000100\n",
      "Timestep 680000\n",
      "mean reward (100 episodes) -17.820000\n",
      "best mean reward -17.820000\n",
      "episodes 631\n",
      "exploration 0.388000\n",
      "learning_rate 0.000100\n",
      "Timestep 690000\n",
      "mean reward (100 episodes) -17.640000\n",
      "best mean reward -17.640000\n",
      "episodes 637\n",
      "exploration 0.379000\n",
      "learning_rate 0.000100\n",
      "Timestep 700000\n",
      "mean reward (100 episodes) -17.490000\n",
      "best mean reward -17.490000\n",
      "episodes 642\n",
      "exploration 0.370000\n",
      "learning_rate 0.000100\n",
      "Timestep 710000\n",
      "mean reward (100 episodes) -17.300000\n",
      "best mean reward -17.300000\n",
      "episodes 647\n",
      "exploration 0.361000\n",
      "learning_rate 0.000100\n",
      "Timestep 720000\n",
      "mean reward (100 episodes) -17.120000\n",
      "best mean reward -17.120000\n",
      "episodes 652\n",
      "exploration 0.352000\n",
      "learning_rate 0.000100\n",
      "Timestep 730000\n",
      "mean reward (100 episodes) -16.920000\n",
      "best mean reward -16.920000\n",
      "episodes 657\n",
      "exploration 0.343000\n",
      "learning_rate 0.000100\n",
      "Timestep 740000\n",
      "mean reward (100 episodes) -16.750000\n",
      "best mean reward -16.750000\n",
      "episodes 661\n",
      "exploration 0.334000\n",
      "learning_rate 0.000100\n",
      "Timestep 750000\n",
      "mean reward (100 episodes) -16.660000\n",
      "best mean reward -16.630000\n",
      "episodes 667\n",
      "exploration 0.325000\n",
      "learning_rate 0.000100\n",
      "Timestep 760000\n",
      "mean reward (100 episodes) -16.470000\n",
      "best mean reward -16.470000\n",
      "episodes 672\n",
      "exploration 0.316000\n",
      "learning_rate 0.000100\n",
      "Timestep 770000\n",
      "mean reward (100 episodes) -16.390000\n",
      "best mean reward -16.390000\n",
      "episodes 677\n",
      "exploration 0.307000\n",
      "learning_rate 0.000100\n",
      "Timestep 780000\n",
      "mean reward (100 episodes) -16.280000\n",
      "best mean reward -16.280000\n",
      "episodes 682\n",
      "exploration 0.298000\n",
      "learning_rate 0.000100\n",
      "Timestep 790000\n",
      "mean reward (100 episodes) -16.070000\n",
      "best mean reward -16.070000\n",
      "episodes 686\n",
      "exploration 0.289000\n",
      "learning_rate 0.000100\n",
      "Timestep 800000\n",
      "mean reward (100 episodes) -15.910000\n",
      "best mean reward -15.910000\n",
      "episodes 692\n",
      "exploration 0.280000\n",
      "learning_rate 0.000100\n",
      "Timestep 810000\n",
      "mean reward (100 episodes) -15.740000\n",
      "best mean reward -15.740000\n",
      "episodes 697\n",
      "exploration 0.271000\n",
      "learning_rate 0.000100\n",
      "Timestep 820000\n",
      "mean reward (100 episodes) -15.630000\n",
      "best mean reward -15.630000\n",
      "episodes 702\n",
      "exploration 0.262000\n",
      "learning_rate 0.000100\n",
      "Timestep 830000\n",
      "mean reward (100 episodes) -15.660000\n",
      "best mean reward -15.630000\n",
      "episodes 708\n",
      "exploration 0.253000\n",
      "learning_rate 0.000100\n",
      "Timestep 840000\n",
      "mean reward (100 episodes) -15.550000\n",
      "best mean reward -15.550000\n",
      "episodes 713\n",
      "exploration 0.244000\n",
      "learning_rate 0.000100\n",
      "Timestep 850000\n",
      "mean reward (100 episodes) -15.440000\n",
      "best mean reward -15.440000\n",
      "episodes 717\n",
      "exploration 0.235000\n",
      "learning_rate 0.000100\n",
      "Timestep 860000\n",
      "mean reward (100 episodes) -15.320000\n",
      "best mean reward -15.280000\n",
      "episodes 722\n",
      "exploration 0.226000\n",
      "learning_rate 0.000100\n",
      "Timestep 870000\n",
      "mean reward (100 episodes) -15.280000\n",
      "best mean reward -15.280000\n",
      "episodes 726\n",
      "exploration 0.217000\n",
      "learning_rate 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 15:14:38,308] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 880000\n",
      "mean reward (100 episodes) -15.090000\n",
      "best mean reward -15.090000\n",
      "episodes 731\n",
      "exploration 0.208000\n",
      "learning_rate 0.000100\n",
      "Timestep 890000\n",
      "mean reward (100 episodes) -14.970000\n",
      "best mean reward -14.970000\n",
      "episodes 736\n",
      "exploration 0.199000\n",
      "learning_rate 0.000100\n",
      "Timestep 900000\n",
      "mean reward (100 episodes) -14.670000\n",
      "best mean reward -14.670000\n",
      "episodes 739\n",
      "exploration 0.190000\n",
      "learning_rate 0.000100\n",
      "Timestep 910000\n",
      "mean reward (100 episodes) -14.500000\n",
      "best mean reward -14.500000\n",
      "episodes 744\n",
      "exploration 0.181000\n",
      "learning_rate 0.000100\n",
      "Timestep 920000\n",
      "mean reward (100 episodes) -14.390000\n",
      "best mean reward -14.390000\n",
      "episodes 748\n",
      "exploration 0.172000\n",
      "learning_rate 0.000100\n",
      "Timestep 930000\n",
      "mean reward (100 episodes) -14.400000\n",
      "best mean reward -14.370000\n",
      "episodes 753\n",
      "exploration 0.163000\n",
      "learning_rate 0.000100\n",
      "Timestep 940000\n",
      "mean reward (100 episodes) -14.340000\n",
      "best mean reward -14.340000\n",
      "episodes 757\n",
      "exploration 0.154000\n",
      "learning_rate 0.000100\n",
      "Timestep 950000\n",
      "mean reward (100 episodes) -14.290000\n",
      "best mean reward -14.290000\n",
      "episodes 762\n",
      "exploration 0.145000\n",
      "learning_rate 0.000100\n",
      "Timestep 960000\n",
      "mean reward (100 episodes) -14.220000\n",
      "best mean reward -14.180000\n",
      "episodes 766\n",
      "exploration 0.136000\n",
      "learning_rate 0.000100\n",
      "Timestep 970000\n",
      "mean reward (100 episodes) -14.030000\n",
      "best mean reward -14.030000\n",
      "episodes 770\n",
      "exploration 0.127000\n",
      "learning_rate 0.000100\n",
      "Timestep 980000\n",
      "mean reward (100 episodes) -13.770000\n",
      "best mean reward -13.770000\n",
      "episodes 775\n",
      "exploration 0.118000\n",
      "learning_rate 0.000100\n",
      "Timestep 990000\n",
      "mean reward (100 episodes) -13.700000\n",
      "best mean reward -13.700000\n",
      "episodes 779\n",
      "exploration 0.109000\n",
      "learning_rate 0.000100\n",
      "Timestep 1000000\n",
      "mean reward (100 episodes) -13.450000\n",
      "best mean reward -13.450000\n",
      "episodes 783\n",
      "exploration 0.100000\n",
      "learning_rate 0.000100\n",
      "Timestep 1010000\n",
      "mean reward (100 episodes) -13.330000\n",
      "best mean reward -13.290000\n",
      "episodes 788\n",
      "exploration 0.099775\n",
      "learning_rate 0.000100\n",
      "Timestep 1020000\n",
      "mean reward (100 episodes) -13.220000\n",
      "best mean reward -13.210000\n",
      "episodes 792\n",
      "exploration 0.099550\n",
      "learning_rate 0.000100\n",
      "Timestep 1030000\n",
      "mean reward (100 episodes) -12.980000\n",
      "best mean reward -12.980000\n",
      "episodes 796\n",
      "exploration 0.099325\n",
      "learning_rate 0.000100\n",
      "Timestep 1040000\n",
      "mean reward (100 episodes) -12.800000\n",
      "best mean reward -12.800000\n",
      "episodes 800\n",
      "exploration 0.099100\n",
      "learning_rate 0.000100\n",
      "Timestep 1050000\n",
      "mean reward (100 episodes) -12.630000\n",
      "best mean reward -12.630000\n",
      "episodes 804\n",
      "exploration 0.098875\n",
      "learning_rate 0.000099\n",
      "Timestep 1060000\n",
      "mean reward (100 episodes) -12.440000\n",
      "best mean reward -12.440000\n",
      "episodes 808\n",
      "exploration 0.098650\n",
      "learning_rate 0.000099\n",
      "Timestep 1070000\n",
      "mean reward (100 episodes) -12.280000\n",
      "best mean reward -12.280000\n",
      "episodes 812\n",
      "exploration 0.098425\n",
      "learning_rate 0.000099\n",
      "Timestep 1080000\n",
      "mean reward (100 episodes) -12.090000\n",
      "best mean reward -12.090000\n",
      "episodes 816\n",
      "exploration 0.098200\n",
      "learning_rate 0.000099\n",
      "Timestep 1090000\n",
      "mean reward (100 episodes) -12.040000\n",
      "best mean reward -11.990000\n",
      "episodes 820\n",
      "exploration 0.097975\n",
      "learning_rate 0.000099\n",
      "Timestep 1100000\n",
      "mean reward (100 episodes) -11.630000\n",
      "best mean reward -11.630000\n",
      "episodes 824\n",
      "exploration 0.097750\n",
      "learning_rate 0.000099\n",
      "Timestep 1110000\n",
      "mean reward (100 episodes) -11.600000\n",
      "best mean reward -11.600000\n",
      "episodes 827\n",
      "exploration 0.097525\n",
      "learning_rate 0.000099\n",
      "Timestep 1120000\n",
      "mean reward (100 episodes) -11.530000\n",
      "best mean reward -11.490000\n",
      "episodes 831\n",
      "exploration 0.097300\n",
      "learning_rate 0.000099\n",
      "Timestep 1130000\n",
      "mean reward (100 episodes) -11.360000\n",
      "best mean reward -11.360000\n",
      "episodes 835\n",
      "exploration 0.097075\n",
      "learning_rate 0.000098\n",
      "Timestep 1140000\n",
      "mean reward (100 episodes) -11.620000\n",
      "best mean reward -11.330000\n",
      "episodes 840\n",
      "exploration 0.096850\n",
      "learning_rate 0.000098\n",
      "Timestep 1150000\n",
      "mean reward (100 episodes) -11.580000\n",
      "best mean reward -11.330000\n",
      "episodes 844\n",
      "exploration 0.096625\n",
      "learning_rate 0.000098\n",
      "Timestep 1160000\n",
      "mean reward (100 episodes) -11.330000\n",
      "best mean reward -11.330000\n",
      "episodes 847\n",
      "exploration 0.096400\n",
      "learning_rate 0.000098\n",
      "Timestep 1170000\n",
      "mean reward (100 episodes) -11.140000\n",
      "best mean reward -11.140000\n",
      "episodes 851\n",
      "exploration 0.096175\n",
      "learning_rate 0.000098\n",
      "Timestep 1180000\n",
      "mean reward (100 episodes) -11.070000\n",
      "best mean reward -11.070000\n",
      "episodes 855\n",
      "exploration 0.095950\n",
      "learning_rate 0.000098\n",
      "Timestep 1190000\n",
      "mean reward (100 episodes) -10.990000\n",
      "best mean reward -10.990000\n",
      "episodes 859\n",
      "exploration 0.095725\n",
      "learning_rate 0.000098\n",
      "Timestep 1200000\n",
      "mean reward (100 episodes) -10.990000\n",
      "best mean reward -10.860000\n",
      "episodes 863\n",
      "exploration 0.095500\n",
      "learning_rate 0.000097\n",
      "Timestep 1210000\n",
      "mean reward (100 episodes) -10.900000\n",
      "best mean reward -10.860000\n",
      "episodes 867\n",
      "exploration 0.095275\n",
      "learning_rate 0.000097\n",
      "Timestep 1220000\n",
      "mean reward (100 episodes) -11.150000\n",
      "best mean reward -10.860000\n",
      "episodes 871\n",
      "exploration 0.095050\n",
      "learning_rate 0.000097\n",
      "Timestep 1230000\n",
      "mean reward (100 episodes) -11.210000\n",
      "best mean reward -10.860000\n",
      "episodes 876\n",
      "exploration 0.094825\n",
      "learning_rate 0.000097\n",
      "Timestep 1240000\n",
      "mean reward (100 episodes) -11.170000\n",
      "best mean reward -10.860000\n",
      "episodes 880\n",
      "exploration 0.094600\n",
      "learning_rate 0.000097\n",
      "Timestep 1250000\n",
      "mean reward (100 episodes) -11.170000\n",
      "best mean reward -10.860000\n",
      "episodes 883\n",
      "exploration 0.094375\n",
      "learning_rate 0.000097\n",
      "Timestep 1260000\n",
      "mean reward (100 episodes) -11.130000\n",
      "best mean reward -10.860000\n",
      "episodes 887\n",
      "exploration 0.094150\n",
      "learning_rate 0.000097\n",
      "Timestep 1270000\n",
      "mean reward (100 episodes) -11.220000\n",
      "best mean reward -10.860000\n",
      "episodes 891\n",
      "exploration 0.093925\n",
      "learning_rate 0.000097\n",
      "Timestep 1280000\n",
      "mean reward (100 episodes) -11.050000\n",
      "best mean reward -10.860000\n",
      "episodes 895\n",
      "exploration 0.093700\n",
      "learning_rate 0.000097\n",
      "Timestep 1290000\n",
      "mean reward (100 episodes) -10.810000\n",
      "best mean reward -10.810000\n",
      "episodes 898\n",
      "exploration 0.093475\n",
      "learning_rate 0.000096\n",
      "Timestep 1300000\n",
      "mean reward (100 episodes) -10.760000\n",
      "best mean reward -10.760000\n",
      "episodes 902\n",
      "exploration 0.093250\n",
      "learning_rate 0.000096\n",
      "Timestep 1310000\n",
      "mean reward (100 episodes) -10.720000\n",
      "best mean reward -10.720000\n",
      "episodes 905\n",
      "exploration 0.093025\n",
      "learning_rate 0.000096\n",
      "Timestep 1320000\n",
      "mean reward (100 episodes) -10.560000\n",
      "best mean reward -10.560000\n",
      "episodes 908\n",
      "exploration 0.092800\n",
      "learning_rate 0.000096\n",
      "Timestep 1330000\n",
      "mean reward (100 episodes) -10.540000\n",
      "best mean reward -10.500000\n",
      "episodes 912\n",
      "exploration 0.092575\n",
      "learning_rate 0.000096\n",
      "Timestep 1340000\n",
      "mean reward (100 episodes) -10.430000\n",
      "best mean reward -10.420000\n",
      "episodes 915\n",
      "exploration 0.092350\n",
      "learning_rate 0.000096\n",
      "Timestep 1350000\n",
      "mean reward (100 episodes) -10.540000\n",
      "best mean reward -10.420000\n",
      "episodes 918\n",
      "exploration 0.092125\n",
      "learning_rate 0.000096\n",
      "Timestep 1360000\n",
      "mean reward (100 episodes) -10.480000\n",
      "best mean reward -10.380000\n",
      "episodes 922\n",
      "exploration 0.091900\n",
      "learning_rate 0.000096\n",
      "Timestep 1370000\n",
      "mean reward (100 episodes) -10.420000\n",
      "best mean reward -10.380000\n",
      "episodes 925\n",
      "exploration 0.091675\n",
      "learning_rate 0.000095\n",
      "Timestep 1380000\n",
      "mean reward (100 episodes) -10.140000\n",
      "best mean reward -10.140000\n",
      "episodes 928\n",
      "exploration 0.091450\n",
      "learning_rate 0.000095\n",
      "Timestep 1390000\n",
      "mean reward (100 episodes) -10.050000\n",
      "best mean reward -10.050000\n",
      "episodes 931\n",
      "exploration 0.091225\n",
      "learning_rate 0.000095\n",
      "Timestep 1400000\n",
      "mean reward (100 episodes) -9.860000\n",
      "best mean reward -9.810000\n",
      "episodes 935\n",
      "exploration 0.091000\n",
      "learning_rate 0.000095\n",
      "Timestep 1410000\n",
      "mean reward (100 episodes) -9.610000\n",
      "best mean reward -9.610000\n",
      "episodes 938\n",
      "exploration 0.090775\n",
      "learning_rate 0.000095\n",
      "Timestep 1420000\n",
      "mean reward (100 episodes) -9.530000\n",
      "best mean reward -9.510000\n",
      "episodes 941\n",
      "exploration 0.090550\n",
      "learning_rate 0.000095\n",
      "Timestep 1430000\n",
      "mean reward (100 episodes) -9.290000\n",
      "best mean reward -9.290000\n",
      "episodes 945\n",
      "exploration 0.090325\n",
      "learning_rate 0.000095\n",
      "Timestep 1440000\n",
      "mean reward (100 episodes) -9.330000\n",
      "best mean reward -9.290000\n",
      "episodes 948\n",
      "exploration 0.090100\n",
      "learning_rate 0.000095\n",
      "Timestep 1450000\n",
      "mean reward (100 episodes) -9.080000\n",
      "best mean reward -9.080000\n",
      "episodes 951\n",
      "exploration 0.089875\n",
      "learning_rate 0.000094\n",
      "Timestep 1460000\n",
      "mean reward (100 episodes) -8.920000\n",
      "best mean reward -8.920000\n",
      "episodes 953\n",
      "exploration 0.089650\n",
      "learning_rate 0.000094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 1470000\n",
      "mean reward (100 episodes) -8.810000\n",
      "best mean reward -8.810000\n",
      "episodes 956\n",
      "exploration 0.089425\n",
      "learning_rate 0.000094\n",
      "Timestep 1480000\n",
      "mean reward (100 episodes) -8.560000\n",
      "best mean reward -8.560000\n",
      "episodes 959\n",
      "exploration 0.089200\n",
      "learning_rate 0.000094\n",
      "Timestep 1490000\n",
      "mean reward (100 episodes) -8.100000\n",
      "best mean reward -8.100000\n",
      "episodes 962\n",
      "exploration 0.088975\n",
      "learning_rate 0.000094\n",
      "Timestep 1500000\n",
      "mean reward (100 episodes) -7.950000\n",
      "best mean reward -7.950000\n",
      "episodes 964\n",
      "exploration 0.088750\n",
      "learning_rate 0.000094\n",
      "Timestep 1510000\n",
      "mean reward (100 episodes) -7.500000\n",
      "best mean reward -7.500000\n",
      "episodes 968\n",
      "exploration 0.088525\n",
      "learning_rate 0.000094\n",
      "Timestep 1520000\n",
      "mean reward (100 episodes) -7.070000\n",
      "best mean reward -7.070000\n",
      "episodes 970\n",
      "exploration 0.088300\n",
      "learning_rate 0.000094\n",
      "Timestep 1530000\n",
      "mean reward (100 episodes) -6.580000\n",
      "best mean reward -6.580000\n",
      "episodes 973\n",
      "exploration 0.088075\n",
      "learning_rate 0.000093\n",
      "Timestep 1540000\n",
      "mean reward (100 episodes) -6.200000\n",
      "best mean reward -6.200000\n",
      "episodes 976\n",
      "exploration 0.087850\n",
      "learning_rate 0.000093\n",
      "Timestep 1550000\n",
      "mean reward (100 episodes) -6.010000\n",
      "best mean reward -6.010000\n",
      "episodes 978\n",
      "exploration 0.087625\n",
      "learning_rate 0.000093\n",
      "Timestep 1560000\n",
      "mean reward (100 episodes) -5.620000\n",
      "best mean reward -5.620000\n",
      "episodes 981\n",
      "exploration 0.087400\n",
      "learning_rate 0.000093\n",
      "Timestep 1570000\n",
      "mean reward (100 episodes) -5.370000\n",
      "best mean reward -5.370000\n",
      "episodes 984\n",
      "exploration 0.087175\n",
      "learning_rate 0.000093\n",
      "Timestep 1580000\n",
      "mean reward (100 episodes) -4.780000\n",
      "best mean reward -4.780000\n",
      "episodes 987\n",
      "exploration 0.086950\n",
      "learning_rate 0.000093\n",
      "Timestep 1590000\n",
      "mean reward (100 episodes) -4.410000\n",
      "best mean reward -4.410000\n",
      "episodes 990\n",
      "exploration 0.086725\n",
      "learning_rate 0.000093\n",
      "Timestep 1600000\n",
      "mean reward (100 episodes) -4.020000\n",
      "best mean reward -4.020000\n",
      "episodes 993\n",
      "exploration 0.086500\n",
      "learning_rate 0.000092\n",
      "Timestep 1610000\n",
      "mean reward (100 episodes) -3.750000\n",
      "best mean reward -3.750000\n",
      "episodes 996\n",
      "exploration 0.086275\n",
      "learning_rate 0.000092\n",
      "Timestep 1620000\n",
      "mean reward (100 episodes) -3.450000\n",
      "best mean reward -3.450000\n",
      "episodes 999\n",
      "exploration 0.086050\n",
      "learning_rate 0.000092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 16:30:51,417] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 1630000\n",
      "mean reward (100 episodes) -3.150000\n",
      "best mean reward -3.150000\n",
      "episodes 1002\n",
      "exploration 0.085825\n",
      "learning_rate 0.000092\n",
      "Timestep 1640000\n",
      "mean reward (100 episodes) -3.070000\n",
      "best mean reward -3.060000\n",
      "episodes 1004\n",
      "exploration 0.085600\n",
      "learning_rate 0.000092\n",
      "Timestep 1650000\n",
      "mean reward (100 episodes) -2.680000\n",
      "best mean reward -2.680000\n",
      "episodes 1008\n",
      "exploration 0.085375\n",
      "learning_rate 0.000092\n",
      "Timestep 1660000\n",
      "mean reward (100 episodes) -2.500000\n",
      "best mean reward -2.500000\n",
      "episodes 1010\n",
      "exploration 0.085150\n",
      "learning_rate 0.000092\n",
      "Timestep 1670000\n",
      "mean reward (100 episodes) -1.960000\n",
      "best mean reward -1.960000\n",
      "episodes 1013\n",
      "exploration 0.084925\n",
      "learning_rate 0.000092\n",
      "Timestep 1680000\n",
      "mean reward (100 episodes) -1.600000\n",
      "best mean reward -1.600000\n",
      "episodes 1016\n",
      "exploration 0.084700\n",
      "learning_rate 0.000092\n",
      "Timestep 1690000\n",
      "mean reward (100 episodes) -1.150000\n",
      "best mean reward -1.150000\n",
      "episodes 1019\n",
      "exploration 0.084475\n",
      "learning_rate 0.000091\n",
      "Timestep 1700000\n",
      "mean reward (100 episodes) -0.700000\n",
      "best mean reward -0.700000\n",
      "episodes 1022\n",
      "exploration 0.084250\n",
      "learning_rate 0.000091\n",
      "Timestep 1710000\n",
      "mean reward (100 episodes) -0.110000\n",
      "best mean reward -0.110000\n",
      "episodes 1025\n",
      "exploration 0.084025\n",
      "learning_rate 0.000091\n",
      "Timestep 1720000\n",
      "mean reward (100 episodes) 0.120000\n",
      "best mean reward 0.120000\n",
      "episodes 1028\n",
      "exploration 0.083800\n",
      "learning_rate 0.000091\n",
      "Timestep 1730000\n",
      "mean reward (100 episodes) 0.560000\n",
      "best mean reward 0.560000\n",
      "episodes 1031\n",
      "exploration 0.083575\n",
      "learning_rate 0.000091\n",
      "Timestep 1740000\n",
      "mean reward (100 episodes) 0.800000\n",
      "best mean reward 0.800000\n",
      "episodes 1034\n",
      "exploration 0.083350\n",
      "learning_rate 0.000091\n",
      "Timestep 1750000\n",
      "mean reward (100 episodes) 1.410000\n",
      "best mean reward 1.410000\n",
      "episodes 1038\n",
      "exploration 0.083125\n",
      "learning_rate 0.000091\n",
      "Timestep 1760000\n",
      "mean reward (100 episodes) 1.850000\n",
      "best mean reward 1.850000\n",
      "episodes 1041\n",
      "exploration 0.082900\n",
      "learning_rate 0.000091\n",
      "Timestep 1770000\n",
      "mean reward (100 episodes) 2.350000\n",
      "best mean reward 2.350000\n",
      "episodes 1044\n",
      "exploration 0.082675\n",
      "learning_rate 0.000090\n",
      "Timestep 1780000\n",
      "mean reward (100 episodes) 2.660000\n",
      "best mean reward 2.660000\n",
      "episodes 1047\n",
      "exploration 0.082450\n",
      "learning_rate 0.000090\n",
      "Timestep 1790000\n",
      "mean reward (100 episodes) 3.190000\n",
      "best mean reward 3.190000\n",
      "episodes 1051\n",
      "exploration 0.082225\n",
      "learning_rate 0.000090\n",
      "Timestep 1800000\n",
      "mean reward (100 episodes) 3.960000\n",
      "best mean reward 3.960000\n",
      "episodes 1055\n",
      "exploration 0.082000\n",
      "learning_rate 0.000090\n",
      "Timestep 1810000\n",
      "mean reward (100 episodes) 4.360000\n",
      "best mean reward 4.360000\n",
      "episodes 1058\n",
      "exploration 0.081775\n",
      "learning_rate 0.000090\n",
      "Timestep 1820000\n",
      "mean reward (100 episodes) 4.930000\n",
      "best mean reward 4.930000\n",
      "episodes 1062\n",
      "exploration 0.081550\n",
      "learning_rate 0.000090\n",
      "Timestep 1830000\n",
      "mean reward (100 episodes) 5.950000\n",
      "best mean reward 5.950000\n",
      "episodes 1067\n",
      "exploration 0.081325\n",
      "learning_rate 0.000090\n",
      "Timestep 1840000\n",
      "mean reward (100 episodes) 6.520000\n",
      "best mean reward 6.520000\n",
      "episodes 1071\n",
      "exploration 0.081100\n",
      "learning_rate 0.000090\n",
      "Timestep 1850000\n",
      "mean reward (100 episodes) 7.260000\n",
      "best mean reward 7.260000\n",
      "episodes 1076\n",
      "exploration 0.080875\n",
      "learning_rate 0.000089\n",
      "Timestep 1860000\n",
      "mean reward (100 episodes) 7.760000\n",
      "best mean reward 7.760000\n",
      "episodes 1080\n",
      "exploration 0.080650\n",
      "learning_rate 0.000089\n",
      "Timestep 1870000\n",
      "mean reward (100 episodes) 8.470000\n",
      "best mean reward 8.470000\n",
      "episodes 1085\n",
      "exploration 0.080425\n",
      "learning_rate 0.000089\n",
      "Timestep 1880000\n",
      "mean reward (100 episodes) 8.700000\n",
      "best mean reward 8.700000\n",
      "episodes 1088\n",
      "exploration 0.080200\n",
      "learning_rate 0.000089\n",
      "Timestep 1890000\n",
      "mean reward (100 episodes) 9.330000\n",
      "best mean reward 9.330000\n",
      "episodes 1092\n",
      "exploration 0.079975\n",
      "learning_rate 0.000089\n",
      "Timestep 1900000\n",
      "mean reward (100 episodes) 10.010000\n",
      "best mean reward 10.010000\n",
      "episodes 1097\n",
      "exploration 0.079750\n",
      "learning_rate 0.000089\n",
      "Timestep 1910000\n",
      "mean reward (100 episodes) 10.370000\n",
      "best mean reward 10.430000\n",
      "episodes 1101\n",
      "exploration 0.079525\n",
      "learning_rate 0.000089\n",
      "Timestep 1920000\n",
      "mean reward (100 episodes) 11.200000\n",
      "best mean reward 11.200000\n",
      "episodes 1105\n",
      "exploration 0.079300\n",
      "learning_rate 0.000089\n",
      "Timestep 1930000\n",
      "mean reward (100 episodes) 11.650000\n",
      "best mean reward 11.650000\n",
      "episodes 1109\n",
      "exploration 0.079075\n",
      "learning_rate 0.000088\n",
      "Timestep 1940000\n",
      "mean reward (100 episodes) 12.100000\n",
      "best mean reward 12.100000\n",
      "episodes 1113\n",
      "exploration 0.078850\n",
      "learning_rate 0.000088\n",
      "Timestep 1950000\n",
      "mean reward (100 episodes) 12.640000\n",
      "best mean reward 12.640000\n",
      "episodes 1118\n",
      "exploration 0.078625\n",
      "learning_rate 0.000088\n",
      "Timestep 1960000\n",
      "mean reward (100 episodes) 13.220000\n",
      "best mean reward 13.220000\n",
      "episodes 1123\n",
      "exploration 0.078400\n",
      "learning_rate 0.000088\n",
      "Timestep 1970000\n",
      "mean reward (100 episodes) 13.480000\n",
      "best mean reward 13.480000\n",
      "episodes 1127\n",
      "exploration 0.078175\n",
      "learning_rate 0.000088\n",
      "Timestep 1980000\n",
      "mean reward (100 episodes) 13.940000\n",
      "best mean reward 13.940000\n",
      "episodes 1132\n",
      "exploration 0.077950\n",
      "learning_rate 0.000088\n",
      "Timestep 1990000\n",
      "mean reward (100 episodes) 14.430000\n",
      "best mean reward 14.430000\n",
      "episodes 1136\n",
      "exploration 0.077725\n",
      "learning_rate 0.000088\n",
      "Timestep 2000000\n",
      "mean reward (100 episodes) 14.680000\n",
      "best mean reward 14.680000\n",
      "episodes 1140\n",
      "exploration 0.077500\n",
      "learning_rate 0.000087\n",
      "Timestep 2010000\n",
      "mean reward (100 episodes) 15.140000\n",
      "best mean reward 15.140000\n",
      "episodes 1145\n",
      "exploration 0.077275\n",
      "learning_rate 0.000087\n",
      "Timestep 2020000\n",
      "mean reward (100 episodes) 15.410000\n",
      "best mean reward 15.420000\n",
      "episodes 1149\n",
      "exploration 0.077050\n",
      "learning_rate 0.000087\n",
      "Timestep 2030000\n",
      "mean reward (100 episodes) 15.510000\n",
      "best mean reward 15.520000\n",
      "episodes 1153\n",
      "exploration 0.076825\n",
      "learning_rate 0.000087\n",
      "Timestep 2040000\n",
      "mean reward (100 episodes) 15.690000\n",
      "best mean reward 15.690000\n",
      "episodes 1158\n",
      "exploration 0.076600\n",
      "learning_rate 0.000087\n",
      "Timestep 2050000\n",
      "mean reward (100 episodes) 15.750000\n",
      "best mean reward 15.750000\n",
      "episodes 1162\n",
      "exploration 0.076375\n",
      "learning_rate 0.000087\n",
      "Timestep 2060000\n",
      "mean reward (100 episodes) 15.710000\n",
      "best mean reward 15.760000\n",
      "episodes 1167\n",
      "exploration 0.076150\n",
      "learning_rate 0.000087\n",
      "Timestep 2070000\n",
      "mean reward (100 episodes) 15.620000\n",
      "best mean reward 15.790000\n",
      "episodes 1171\n",
      "exploration 0.075925\n",
      "learning_rate 0.000087\n",
      "Timestep 2080000\n",
      "mean reward (100 episodes) 15.660000\n",
      "best mean reward 15.790000\n",
      "episodes 1175\n",
      "exploration 0.075700\n",
      "learning_rate 0.000087\n",
      "Timestep 2090000\n",
      "mean reward (100 episodes) 15.780000\n",
      "best mean reward 15.790000\n",
      "episodes 1180\n",
      "exploration 0.075475\n",
      "learning_rate 0.000086\n",
      "Timestep 2100000\n",
      "mean reward (100 episodes) 15.810000\n",
      "best mean reward 15.810000\n",
      "episodes 1185\n",
      "exploration 0.075250\n",
      "learning_rate 0.000086\n",
      "Timestep 2110000\n",
      "mean reward (100 episodes) 16.120000\n",
      "best mean reward 16.120000\n",
      "episodes 1190\n",
      "exploration 0.075025\n",
      "learning_rate 0.000086\n",
      "Timestep 2120000\n",
      "mean reward (100 episodes) 16.140000\n",
      "best mean reward 16.160000\n",
      "episodes 1194\n",
      "exploration 0.074800\n",
      "learning_rate 0.000086\n",
      "Timestep 2130000\n",
      "mean reward (100 episodes) 16.160000\n",
      "best mean reward 16.200000\n",
      "episodes 1198\n",
      "exploration 0.074575\n",
      "learning_rate 0.000086\n",
      "Timestep 2140000\n",
      "mean reward (100 episodes) 16.160000\n",
      "best mean reward 16.200000\n",
      "episodes 1203\n",
      "exploration 0.074350\n",
      "learning_rate 0.000086\n",
      "Timestep 2150000\n",
      "mean reward (100 episodes) 16.280000\n",
      "best mean reward 16.280000\n",
      "episodes 1207\n",
      "exploration 0.074125\n",
      "learning_rate 0.000086\n",
      "Timestep 2160000\n",
      "mean reward (100 episodes) 16.250000\n",
      "best mean reward 16.280000\n",
      "episodes 1211\n",
      "exploration 0.073900\n",
      "learning_rate 0.000086\n",
      "Timestep 2170000\n",
      "mean reward (100 episodes) 16.320000\n",
      "best mean reward 16.320000\n",
      "episodes 1216\n",
      "exploration 0.073675\n",
      "learning_rate 0.000085\n",
      "Timestep 2180000\n",
      "mean reward (100 episodes) 16.180000\n",
      "best mean reward 16.320000\n",
      "episodes 1220\n",
      "exploration 0.073450\n",
      "learning_rate 0.000085\n",
      "Timestep 2190000\n",
      "mean reward (100 episodes) 16.210000\n",
      "best mean reward 16.320000\n",
      "episodes 1225\n",
      "exploration 0.073225\n",
      "learning_rate 0.000085\n",
      "Timestep 2200000\n",
      "mean reward (100 episodes) 16.230000\n",
      "best mean reward 16.320000\n",
      "episodes 1229\n",
      "exploration 0.073000\n",
      "learning_rate 0.000085\n",
      "Timestep 2210000\n",
      "mean reward (100 episodes) 16.240000\n",
      "best mean reward 16.320000\n",
      "episodes 1233\n",
      "exploration 0.072775\n",
      "learning_rate 0.000085\n",
      "Timestep 2220000\n",
      "mean reward (100 episodes) 16.150000\n",
      "best mean reward 16.320000\n",
      "episodes 1238\n",
      "exploration 0.072550\n",
      "learning_rate 0.000085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 2230000\n",
      "mean reward (100 episodes) 16.100000\n",
      "best mean reward 16.320000\n",
      "episodes 1241\n",
      "exploration 0.072325\n",
      "learning_rate 0.000085\n",
      "Timestep 2240000\n",
      "mean reward (100 episodes) 16.140000\n",
      "best mean reward 16.320000\n",
      "episodes 1246\n",
      "exploration 0.072100\n",
      "learning_rate 0.000085\n",
      "Timestep 2250000\n",
      "mean reward (100 episodes) 16.080000\n",
      "best mean reward 16.320000\n",
      "episodes 1250\n",
      "exploration 0.071875\n",
      "learning_rate 0.000084\n",
      "Timestep 2260000\n",
      "mean reward (100 episodes) 16.190000\n",
      "best mean reward 16.320000\n",
      "episodes 1255\n",
      "exploration 0.071650\n",
      "learning_rate 0.000084\n",
      "Timestep 2270000\n",
      "mean reward (100 episodes) 16.200000\n",
      "best mean reward 16.320000\n",
      "episodes 1259\n",
      "exploration 0.071425\n",
      "learning_rate 0.000084\n",
      "Timestep 2280000\n",
      "mean reward (100 episodes) 16.280000\n",
      "best mean reward 16.320000\n",
      "episodes 1264\n",
      "exploration 0.071200\n",
      "learning_rate 0.000084\n",
      "Timestep 2290000\n",
      "mean reward (100 episodes) 16.490000\n",
      "best mean reward 16.490000\n",
      "episodes 1270\n",
      "exploration 0.070975\n",
      "learning_rate 0.000084\n",
      "Timestep 2300000\n",
      "mean reward (100 episodes) 16.600000\n",
      "best mean reward 16.600000\n",
      "episodes 1275\n",
      "exploration 0.070750\n",
      "learning_rate 0.000084\n",
      "Timestep 2310000\n",
      "mean reward (100 episodes) 16.600000\n",
      "best mean reward 16.610000\n",
      "episodes 1279\n",
      "exploration 0.070525\n",
      "learning_rate 0.000084\n",
      "Timestep 2320000\n",
      "mean reward (100 episodes) 16.690000\n",
      "best mean reward 16.690000\n",
      "episodes 1284\n",
      "exploration 0.070300\n",
      "learning_rate 0.000083\n",
      "Timestep 2330000\n",
      "mean reward (100 episodes) 16.620000\n",
      "best mean reward 16.690000\n",
      "episodes 1289\n",
      "exploration 0.070075\n",
      "learning_rate 0.000083\n",
      "Timestep 2340000\n",
      "mean reward (100 episodes) 16.660000\n",
      "best mean reward 16.690000\n",
      "episodes 1294\n",
      "exploration 0.069850\n",
      "learning_rate 0.000083\n",
      "Timestep 2350000\n",
      "mean reward (100 episodes) 16.840000\n",
      "best mean reward 16.840000\n",
      "episodes 1299\n",
      "exploration 0.069625\n",
      "learning_rate 0.000083\n",
      "Timestep 2360000\n",
      "mean reward (100 episodes) 16.940000\n",
      "best mean reward 16.940000\n",
      "episodes 1304\n",
      "exploration 0.069400\n",
      "learning_rate 0.000083\n",
      "Timestep 2370000\n",
      "mean reward (100 episodes) 17.060000\n",
      "best mean reward 17.060000\n",
      "episodes 1309\n",
      "exploration 0.069175\n",
      "learning_rate 0.000083\n",
      "Timestep 2380000\n",
      "mean reward (100 episodes) 17.020000\n",
      "best mean reward 17.060000\n",
      "episodes 1313\n",
      "exploration 0.068950\n",
      "learning_rate 0.000083\n",
      "Timestep 2390000\n",
      "mean reward (100 episodes) 17.050000\n",
      "best mean reward 17.060000\n",
      "episodes 1318\n",
      "exploration 0.068725\n",
      "learning_rate 0.000083\n",
      "Timestep 2400000\n",
      "mean reward (100 episodes) 17.180000\n",
      "best mean reward 17.180000\n",
      "episodes 1323\n",
      "exploration 0.068500\n",
      "learning_rate 0.000082\n",
      "Timestep 2410000\n",
      "mean reward (100 episodes) 17.200000\n",
      "best mean reward 17.230000\n",
      "episodes 1328\n",
      "exploration 0.068275\n",
      "learning_rate 0.000082\n",
      "Timestep 2420000\n",
      "mean reward (100 episodes) 17.290000\n",
      "best mean reward 17.290000\n",
      "episodes 1332\n",
      "exploration 0.068050\n",
      "learning_rate 0.000082\n",
      "Timestep 2430000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 17.430000\n",
      "episodes 1337\n",
      "exploration 0.067825\n",
      "learning_rate 0.000082\n",
      "Timestep 2440000\n",
      "mean reward (100 episodes) 17.590000\n",
      "best mean reward 17.590000\n",
      "episodes 1342\n",
      "exploration 0.067600\n",
      "learning_rate 0.000082\n",
      "Timestep 2450000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 17.620000\n",
      "episodes 1346\n",
      "exploration 0.067375\n",
      "learning_rate 0.000082\n",
      "Timestep 2460000\n",
      "mean reward (100 episodes) 17.640000\n",
      "best mean reward 17.680000\n",
      "episodes 1351\n",
      "exploration 0.067150\n",
      "learning_rate 0.000082\n",
      "Timestep 2470000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 17.730000\n",
      "episodes 1356\n",
      "exploration 0.066925\n",
      "learning_rate 0.000082\n",
      "Timestep 2480000\n",
      "mean reward (100 episodes) 17.710000\n",
      "best mean reward 17.730000\n",
      "episodes 1359\n",
      "exploration 0.066700\n",
      "learning_rate 0.000082\n",
      "Timestep 2490000\n",
      "mean reward (100 episodes) 17.720000\n",
      "best mean reward 17.730000\n",
      "episodes 1364\n",
      "exploration 0.066475\n",
      "learning_rate 0.000081\n",
      "Timestep 2500000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 17.730000\n",
      "episodes 1368\n",
      "exploration 0.066250\n",
      "learning_rate 0.000081\n",
      "Timestep 2510000\n",
      "mean reward (100 episodes) 17.550000\n",
      "best mean reward 17.730000\n",
      "episodes 1373\n",
      "exploration 0.066025\n",
      "learning_rate 0.000081\n",
      "Timestep 2520000\n",
      "mean reward (100 episodes) 17.600000\n",
      "best mean reward 17.730000\n",
      "episodes 1378\n",
      "exploration 0.065800\n",
      "learning_rate 0.000081\n",
      "Timestep 2530000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 17.730000\n",
      "episodes 1383\n",
      "exploration 0.065575\n",
      "learning_rate 0.000081\n",
      "Timestep 2540000\n",
      "mean reward (100 episodes) 17.470000\n",
      "best mean reward 17.730000\n",
      "episodes 1387\n",
      "exploration 0.065350\n",
      "learning_rate 0.000081\n",
      "Timestep 2550000\n",
      "mean reward (100 episodes) 17.330000\n",
      "best mean reward 17.730000\n",
      "episodes 1392\n",
      "exploration 0.065125\n",
      "learning_rate 0.000081\n",
      "Timestep 2560000\n",
      "mean reward (100 episodes) 17.310000\n",
      "best mean reward 17.730000\n",
      "episodes 1396\n",
      "exploration 0.064900\n",
      "learning_rate 0.000081\n",
      "Timestep 2570000\n",
      "mean reward (100 episodes) 17.340000\n",
      "best mean reward 17.730000\n",
      "episodes 1401\n",
      "exploration 0.064675\n",
      "learning_rate 0.000080\n",
      "Timestep 2580000\n",
      "mean reward (100 episodes) 17.280000\n",
      "best mean reward 17.730000\n",
      "episodes 1406\n",
      "exploration 0.064450\n",
      "learning_rate 0.000080\n",
      "Timestep 2590000\n",
      "mean reward (100 episodes) 17.370000\n",
      "best mean reward 17.730000\n",
      "episodes 1411\n",
      "exploration 0.064225\n",
      "learning_rate 0.000080\n",
      "Timestep 2600000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 17.730000\n",
      "episodes 1416\n",
      "exploration 0.064000\n",
      "learning_rate 0.000080\n",
      "Timestep 2610000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 17.730000\n",
      "episodes 1421\n",
      "exploration 0.063775\n",
      "learning_rate 0.000080\n",
      "Timestep 2620000\n",
      "mean reward (100 episodes) 17.400000\n",
      "best mean reward 17.730000\n",
      "episodes 1426\n",
      "exploration 0.063550\n",
      "learning_rate 0.000080\n",
      "Timestep 2630000\n",
      "mean reward (100 episodes) 17.380000\n",
      "best mean reward 17.730000\n",
      "episodes 1431\n",
      "exploration 0.063325\n",
      "learning_rate 0.000080\n",
      "Timestep 2640000\n",
      "mean reward (100 episodes) 17.370000\n",
      "best mean reward 17.730000\n",
      "episodes 1435\n",
      "exploration 0.063100\n",
      "learning_rate 0.000080\n",
      "Timestep 2650000\n",
      "mean reward (100 episodes) 17.420000\n",
      "best mean reward 17.730000\n",
      "episodes 1440\n",
      "exploration 0.062875\n",
      "learning_rate 0.000079\n",
      "Timestep 2660000\n",
      "mean reward (100 episodes) 17.500000\n",
      "best mean reward 17.730000\n",
      "episodes 1445\n",
      "exploration 0.062650\n",
      "learning_rate 0.000079\n",
      "Timestep 2670000\n",
      "mean reward (100 episodes) 17.560000\n",
      "best mean reward 17.730000\n",
      "episodes 1450\n",
      "exploration 0.062425\n",
      "learning_rate 0.000079\n",
      "Timestep 2680000\n",
      "mean reward (100 episodes) 17.570000\n",
      "best mean reward 17.730000\n",
      "episodes 1455\n",
      "exploration 0.062200\n",
      "learning_rate 0.000079\n",
      "Timestep 2690000\n",
      "mean reward (100 episodes) 17.730000\n",
      "best mean reward 17.730000\n",
      "episodes 1460\n",
      "exploration 0.061975\n",
      "learning_rate 0.000079\n",
      "Timestep 2700000\n",
      "mean reward (100 episodes) 17.740000\n",
      "best mean reward 17.740000\n",
      "episodes 1465\n",
      "exploration 0.061750\n",
      "learning_rate 0.000079\n",
      "Timestep 2710000\n",
      "mean reward (100 episodes) 17.840000\n",
      "best mean reward 17.850000\n",
      "episodes 1470\n",
      "exploration 0.061525\n",
      "learning_rate 0.000079\n",
      "Timestep 2720000\n",
      "mean reward (100 episodes) 17.940000\n",
      "best mean reward 17.940000\n",
      "episodes 1476\n",
      "exploration 0.061300\n",
      "learning_rate 0.000079\n",
      "Timestep 2730000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.000000\n",
      "episodes 1481\n",
      "exploration 0.061075\n",
      "learning_rate 0.000078\n",
      "Timestep 2740000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.090000\n",
      "episodes 1486\n",
      "exploration 0.060850\n",
      "learning_rate 0.000078\n",
      "Timestep 2750000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.250000\n",
      "episodes 1491\n",
      "exploration 0.060625\n",
      "learning_rate 0.000078\n",
      "Timestep 2760000\n",
      "mean reward (100 episodes) 18.210000\n",
      "best mean reward 18.250000\n",
      "episodes 1495\n",
      "exploration 0.060400\n",
      "learning_rate 0.000078\n",
      "Timestep 2770000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.250000\n",
      "episodes 1501\n",
      "exploration 0.060175\n",
      "learning_rate 0.000078\n",
      "Timestep 2780000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.330000\n",
      "episodes 1505\n",
      "exploration 0.059950\n",
      "learning_rate 0.000078\n",
      "Timestep 2790000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.330000\n",
      "episodes 1510\n",
      "exploration 0.059725\n",
      "learning_rate 0.000078\n",
      "Timestep 2800000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.330000\n",
      "episodes 1515\n",
      "exploration 0.059500\n",
      "learning_rate 0.000077\n",
      "Timestep 2810000\n",
      "mean reward (100 episodes) 18.240000\n",
      "best mean reward 18.330000\n",
      "episodes 1519\n",
      "exploration 0.059275\n",
      "learning_rate 0.000077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 2820000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.330000\n",
      "episodes 1524\n",
      "exploration 0.059050\n",
      "learning_rate 0.000077\n",
      "Timestep 2830000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.340000\n",
      "episodes 1529\n",
      "exploration 0.058825\n",
      "learning_rate 0.000077\n",
      "Timestep 2840000\n",
      "mean reward (100 episodes) 18.300000\n",
      "best mean reward 18.340000\n",
      "episodes 1533\n",
      "exploration 0.058600\n",
      "learning_rate 0.000077\n",
      "Timestep 2850000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.340000\n",
      "episodes 1538\n",
      "exploration 0.058375\n",
      "learning_rate 0.000077\n",
      "Timestep 2860000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.360000\n",
      "episodes 1543\n",
      "exploration 0.058150\n",
      "learning_rate 0.000077\n",
      "Timestep 2870000\n",
      "mean reward (100 episodes) 18.330000\n",
      "best mean reward 18.360000\n",
      "episodes 1547\n",
      "exploration 0.057925\n",
      "learning_rate 0.000077\n",
      "Timestep 2880000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.370000\n",
      "episodes 1552\n",
      "exploration 0.057700\n",
      "learning_rate 0.000077\n",
      "Timestep 2890000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.370000\n",
      "episodes 1556\n",
      "exploration 0.057475\n",
      "learning_rate 0.000076\n",
      "Timestep 2900000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.370000\n",
      "episodes 1560\n",
      "exploration 0.057250\n",
      "learning_rate 0.000076\n",
      "Timestep 2910000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.370000\n",
      "episodes 1565\n",
      "exploration 0.057025\n",
      "learning_rate 0.000076\n",
      "Timestep 2920000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.370000\n",
      "episodes 1570\n",
      "exploration 0.056800\n",
      "learning_rate 0.000076\n",
      "Timestep 2930000\n",
      "mean reward (100 episodes) 17.970000\n",
      "best mean reward 18.370000\n",
      "episodes 1575\n",
      "exploration 0.056575\n",
      "learning_rate 0.000076\n",
      "Timestep 2940000\n",
      "mean reward (100 episodes) 18.000000\n",
      "best mean reward 18.370000\n",
      "episodes 1580\n",
      "exploration 0.056350\n",
      "learning_rate 0.000076\n",
      "Timestep 2950000\n",
      "mean reward (100 episodes) 17.920000\n",
      "best mean reward 18.370000\n",
      "episodes 1584\n",
      "exploration 0.056125\n",
      "learning_rate 0.000076\n",
      "Timestep 2960000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.370000\n",
      "episodes 1590\n",
      "exploration 0.055900\n",
      "learning_rate 0.000076\n",
      "Timestep 2970000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.370000\n",
      "episodes 1595\n",
      "exploration 0.055675\n",
      "learning_rate 0.000075\n",
      "Timestep 2980000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.370000\n",
      "episodes 1600\n",
      "exploration 0.055450\n",
      "learning_rate 0.000075\n",
      "Timestep 2990000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.370000\n",
      "episodes 1605\n",
      "exploration 0.055225\n",
      "learning_rate 0.000075\n",
      "Timestep 3000000\n",
      "mean reward (100 episodes) 18.040000\n",
      "best mean reward 18.370000\n",
      "episodes 1610\n",
      "exploration 0.055000\n",
      "learning_rate 0.000075\n",
      "Timestep 3010000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.370000\n",
      "episodes 1615\n",
      "exploration 0.054775\n",
      "learning_rate 0.000075\n",
      "Timestep 3020000\n",
      "mean reward (100 episodes) 18.050000\n",
      "best mean reward 18.370000\n",
      "episodes 1620\n",
      "exploration 0.054550\n",
      "learning_rate 0.000075\n",
      "Timestep 3030000\n",
      "mean reward (100 episodes) 18.010000\n",
      "best mean reward 18.370000\n",
      "episodes 1624\n",
      "exploration 0.054325\n",
      "learning_rate 0.000075\n",
      "Timestep 3040000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.370000\n",
      "episodes 1630\n",
      "exploration 0.054100\n",
      "learning_rate 0.000074\n",
      "Timestep 3050000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.370000\n",
      "episodes 1635\n",
      "exploration 0.053875\n",
      "learning_rate 0.000074\n",
      "Timestep 3060000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.370000\n",
      "episodes 1640\n",
      "exploration 0.053650\n",
      "learning_rate 0.000074\n",
      "Timestep 3070000\n",
      "mean reward (100 episodes) 18.100000\n",
      "best mean reward 18.370000\n",
      "episodes 1645\n",
      "exploration 0.053425\n",
      "learning_rate 0.000074\n",
      "Timestep 3080000\n",
      "mean reward (100 episodes) 18.160000\n",
      "best mean reward 18.370000\n",
      "episodes 1650\n",
      "exploration 0.053200\n",
      "learning_rate 0.000074\n",
      "Timestep 3090000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.370000\n",
      "episodes 1655\n",
      "exploration 0.052975\n",
      "learning_rate 0.000074\n",
      "Timestep 3100000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.370000\n",
      "episodes 1659\n",
      "exploration 0.052750\n",
      "learning_rate 0.000074\n",
      "Timestep 3110000\n",
      "mean reward (100 episodes) 18.340000\n",
      "best mean reward 18.370000\n",
      "episodes 1665\n",
      "exploration 0.052525\n",
      "learning_rate 0.000074\n",
      "Timestep 3120000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.370000\n",
      "episodes 1669\n",
      "exploration 0.052300\n",
      "learning_rate 0.000073\n",
      "Timestep 3130000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.370000\n",
      "episodes 1674\n",
      "exploration 0.052075\n",
      "learning_rate 0.000073\n",
      "Timestep 3140000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.370000\n",
      "episodes 1679\n",
      "exploration 0.051850\n",
      "learning_rate 0.000073\n",
      "Timestep 3150000\n",
      "mean reward (100 episodes) 18.230000\n",
      "best mean reward 18.370000\n",
      "episodes 1684\n",
      "exploration 0.051625\n",
      "learning_rate 0.000073\n",
      "Timestep 3160000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.370000\n",
      "episodes 1688\n",
      "exploration 0.051400\n",
      "learning_rate 0.000073\n",
      "Timestep 3170000\n",
      "mean reward (100 episodes) 18.130000\n",
      "best mean reward 18.370000\n",
      "episodes 1693\n",
      "exploration 0.051175\n",
      "learning_rate 0.000073\n",
      "Timestep 3180000\n",
      "mean reward (100 episodes) 18.060000\n",
      "best mean reward 18.370000\n",
      "episodes 1698\n",
      "exploration 0.050950\n",
      "learning_rate 0.000073\n",
      "Timestep 3190000\n",
      "mean reward (100 episodes) 18.070000\n",
      "best mean reward 18.370000\n",
      "episodes 1703\n",
      "exploration 0.050725\n",
      "learning_rate 0.000073\n",
      "Timestep 3200000\n",
      "mean reward (100 episodes) 18.020000\n",
      "best mean reward 18.370000\n",
      "episodes 1708\n",
      "exploration 0.050500\n",
      "learning_rate 0.000073\n",
      "Timestep 3210000\n",
      "mean reward (100 episodes) 18.090000\n",
      "best mean reward 18.370000\n",
      "episodes 1713\n",
      "exploration 0.050275\n",
      "learning_rate 0.000072\n",
      "Timestep 3220000\n",
      "mean reward (100 episodes) 18.110000\n",
      "best mean reward 18.370000\n",
      "episodes 1718\n",
      "exploration 0.050050\n",
      "learning_rate 0.000072\n",
      "Timestep 3230000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.370000\n",
      "episodes 1724\n",
      "exploration 0.049825\n",
      "learning_rate 0.000072\n",
      "Timestep 3240000\n",
      "mean reward (100 episodes) 18.280000\n",
      "best mean reward 18.370000\n",
      "episodes 1728\n",
      "exploration 0.049600\n",
      "learning_rate 0.000072\n",
      "Timestep 3250000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.370000\n",
      "episodes 1734\n",
      "exploration 0.049375\n",
      "learning_rate 0.000072\n",
      "Timestep 3260000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.370000\n",
      "episodes 1739\n",
      "exploration 0.049150\n",
      "learning_rate 0.000072\n",
      "Timestep 3270000\n",
      "mean reward (100 episodes) 18.220000\n",
      "best mean reward 18.370000\n",
      "episodes 1744\n",
      "exploration 0.048925\n",
      "learning_rate 0.000072\n",
      "Timestep 3280000\n",
      "mean reward (100 episodes) 18.180000\n",
      "best mean reward 18.370000\n",
      "episodes 1749\n",
      "exploration 0.048700\n",
      "learning_rate 0.000072\n",
      "Timestep 3290000\n",
      "mean reward (100 episodes) 18.200000\n",
      "best mean reward 18.370000\n",
      "episodes 1754\n",
      "exploration 0.048475\n",
      "learning_rate 0.000071\n",
      "Timestep 3300000\n",
      "mean reward (100 episodes) 18.120000\n",
      "best mean reward 18.370000\n",
      "episodes 1758\n",
      "exploration 0.048250\n",
      "learning_rate 0.000071\n",
      "Timestep 3310000\n",
      "mean reward (100 episodes) 18.150000\n",
      "best mean reward 18.370000\n",
      "episodes 1763\n",
      "exploration 0.048025\n",
      "learning_rate 0.000071\n",
      "Timestep 3320000\n",
      "mean reward (100 episodes) 18.350000\n",
      "best mean reward 18.370000\n",
      "episodes 1769\n",
      "exploration 0.047800\n",
      "learning_rate 0.000071\n",
      "Timestep 3330000\n",
      "mean reward (100 episodes) 18.270000\n",
      "best mean reward 18.370000\n",
      "episodes 1773\n",
      "exploration 0.047575\n",
      "learning_rate 0.000071\n",
      "Timestep 3340000\n",
      "mean reward (100 episodes) 18.290000\n",
      "best mean reward 18.370000\n",
      "episodes 1778\n",
      "exploration 0.047350\n",
      "learning_rate 0.000071\n",
      "Timestep 3350000\n",
      "mean reward (100 episodes) 18.320000\n",
      "best mean reward 18.370000\n",
      "episodes 1783\n",
      "exploration 0.047125\n",
      "learning_rate 0.000071\n",
      "Timestep 3360000\n",
      "mean reward (100 episodes) 18.380000\n",
      "best mean reward 18.400000\n",
      "episodes 1788\n",
      "exploration 0.046900\n",
      "learning_rate 0.000071\n",
      "Timestep 3370000\n",
      "mean reward (100 episodes) 18.560000\n",
      "best mean reward 18.560000\n",
      "episodes 1794\n",
      "exploration 0.046675\n",
      "learning_rate 0.000070\n",
      "Timestep 3380000\n",
      "mean reward (100 episodes) 18.570000\n",
      "best mean reward 18.630000\n",
      "episodes 1799\n",
      "exploration 0.046450\n",
      "learning_rate 0.000070\n",
      "Timestep 3390000\n",
      "mean reward (100 episodes) 18.590000\n",
      "best mean reward 18.630000\n",
      "episodes 1803\n",
      "exploration 0.046225\n",
      "learning_rate 0.000070\n",
      "Timestep 3400000\n",
      "mean reward (100 episodes) 18.660000\n",
      "best mean reward 18.660000\n",
      "episodes 1808\n",
      "exploration 0.046000\n",
      "learning_rate 0.000070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 3410000\n",
      "mean reward (100 episodes) 18.600000\n",
      "best mean reward 18.660000\n",
      "episodes 1814\n",
      "exploration 0.045775\n",
      "learning_rate 0.000070\n",
      "Timestep 3420000\n",
      "mean reward (100 episodes) 18.620000\n",
      "best mean reward 18.660000\n",
      "episodes 1819\n",
      "exploration 0.045550\n",
      "learning_rate 0.000070\n",
      "Timestep 3430000\n",
      "mean reward (100 episodes) 18.560000\n",
      "best mean reward 18.660000\n",
      "episodes 1824\n",
      "exploration 0.045325\n",
      "learning_rate 0.000070\n",
      "Timestep 3440000\n",
      "mean reward (100 episodes) 18.550000\n",
      "best mean reward 18.660000\n",
      "episodes 1829\n",
      "exploration 0.045100\n",
      "learning_rate 0.000070\n",
      "Timestep 3450000\n",
      "mean reward (100 episodes) 18.530000\n",
      "best mean reward 18.660000\n",
      "episodes 1834\n",
      "exploration 0.044875\n",
      "learning_rate 0.000069\n",
      "Timestep 3460000\n",
      "mean reward (100 episodes) 18.670000\n",
      "best mean reward 18.670000\n",
      "episodes 1840\n",
      "exploration 0.044650\n",
      "learning_rate 0.000069\n",
      "Timestep 3470000\n",
      "mean reward (100 episodes) 18.690000\n",
      "best mean reward 18.690000\n",
      "episodes 1844\n",
      "exploration 0.044425\n",
      "learning_rate 0.000069\n",
      "Timestep 3480000\n",
      "mean reward (100 episodes) 18.710000\n",
      "best mean reward 18.730000\n",
      "episodes 1850\n",
      "exploration 0.044200\n",
      "learning_rate 0.000069\n",
      "Timestep 3490000\n",
      "mean reward (100 episodes) 18.780000\n",
      "best mean reward 18.780000\n",
      "episodes 1855\n",
      "exploration 0.043975\n",
      "learning_rate 0.000069\n",
      "Timestep 3500000\n",
      "mean reward (100 episodes) 18.870000\n",
      "best mean reward 18.890000\n",
      "episodes 1860\n",
      "exploration 0.043750\n",
      "learning_rate 0.000069\n",
      "Timestep 3510000\n",
      "mean reward (100 episodes) 18.860000\n",
      "best mean reward 18.900000\n",
      "episodes 1865\n",
      "exploration 0.043525\n",
      "learning_rate 0.000069\n",
      "Timestep 3520000\n",
      "mean reward (100 episodes) 18.860000\n",
      "best mean reward 18.900000\n",
      "episodes 1871\n",
      "exploration 0.043300\n",
      "learning_rate 0.000069\n",
      "Timestep 3530000\n",
      "mean reward (100 episodes) 19.010000\n",
      "best mean reward 19.010000\n",
      "episodes 1876\n",
      "exploration 0.043075\n",
      "learning_rate 0.000068\n",
      "Timestep 3540000\n",
      "mean reward (100 episodes) 18.920000\n",
      "best mean reward 19.010000\n",
      "episodes 1881\n",
      "exploration 0.042850\n",
      "learning_rate 0.000068\n",
      "Timestep 3550000\n",
      "mean reward (100 episodes) 19.040000\n",
      "best mean reward 19.040000\n",
      "episodes 1887\n",
      "exploration 0.042625\n",
      "learning_rate 0.000068\n",
      "Timestep 3560000\n",
      "mean reward (100 episodes) 19.080000\n",
      "best mean reward 19.080000\n",
      "episodes 1892\n",
      "exploration 0.042400\n",
      "learning_rate 0.000068\n",
      "Timestep 3570000\n",
      "mean reward (100 episodes) 19.180000\n",
      "best mean reward 19.180000\n",
      "episodes 1898\n",
      "exploration 0.042175\n",
      "learning_rate 0.000068\n",
      "Timestep 3580000\n",
      "mean reward (100 episodes) 19.210000\n",
      "best mean reward 19.210000\n",
      "episodes 1903\n",
      "exploration 0.041950\n",
      "learning_rate 0.000068\n",
      "Timestep 3590000\n",
      "mean reward (100 episodes) 19.200000\n",
      "best mean reward 19.210000\n",
      "episodes 1908\n",
      "exploration 0.041725\n",
      "learning_rate 0.000068\n",
      "Timestep 3600000\n",
      "mean reward (100 episodes) 19.110000\n",
      "best mean reward 19.210000\n",
      "episodes 1913\n",
      "exploration 0.041500\n",
      "learning_rate 0.000068\n",
      "Timestep 3610000\n",
      "mean reward (100 episodes) 19.100000\n",
      "best mean reward 19.210000\n",
      "episodes 1918\n",
      "exploration 0.041275\n",
      "learning_rate 0.000067\n",
      "Timestep 3620000\n",
      "mean reward (100 episodes) 19.200000\n",
      "best mean reward 19.210000\n",
      "episodes 1924\n",
      "exploration 0.041050\n",
      "learning_rate 0.000067\n",
      "Timestep 3630000\n",
      "mean reward (100 episodes) 19.280000\n",
      "best mean reward 19.290000\n",
      "episodes 1929\n",
      "exploration 0.040825\n",
      "learning_rate 0.000067\n",
      "Timestep 3640000\n",
      "mean reward (100 episodes) 19.320000\n",
      "best mean reward 19.320000\n",
      "episodes 1935\n",
      "exploration 0.040600\n",
      "learning_rate 0.000067\n",
      "Timestep 3650000\n",
      "mean reward (100 episodes) 19.280000\n",
      "best mean reward 19.320000\n",
      "episodes 1940\n",
      "exploration 0.040375\n",
      "learning_rate 0.000067\n",
      "Timestep 3660000\n",
      "mean reward (100 episodes) 19.290000\n",
      "best mean reward 19.320000\n",
      "episodes 1945\n",
      "exploration 0.040150\n",
      "learning_rate 0.000067\n",
      "Timestep 3670000\n",
      "mean reward (100 episodes) 19.400000\n",
      "best mean reward 19.400000\n",
      "episodes 1951\n",
      "exploration 0.039925\n",
      "learning_rate 0.000067\n",
      "Timestep 3680000\n",
      "mean reward (100 episodes) 19.380000\n",
      "best mean reward 19.400000\n",
      "episodes 1956\n",
      "exploration 0.039700\n",
      "learning_rate 0.000067\n",
      "Timestep 3690000\n",
      "mean reward (100 episodes) 19.420000\n",
      "best mean reward 19.420000\n",
      "episodes 1962\n",
      "exploration 0.039475\n",
      "learning_rate 0.000066\n",
      "Timestep 3700000\n",
      "mean reward (100 episodes) 19.490000\n",
      "best mean reward 19.490000\n",
      "episodes 1967\n",
      "exploration 0.039250\n",
      "learning_rate 0.000066\n",
      "Timestep 3710000\n",
      "mean reward (100 episodes) 19.490000\n",
      "best mean reward 19.500000\n",
      "episodes 1972\n",
      "exploration 0.039025\n",
      "learning_rate 0.000066\n",
      "Timestep 3720000\n",
      "mean reward (100 episodes) 19.590000\n",
      "best mean reward 19.590000\n",
      "episodes 1978\n",
      "exploration 0.038800\n",
      "learning_rate 0.000066\n",
      "Timestep 3730000\n",
      "mean reward (100 episodes) 19.610000\n",
      "best mean reward 19.610000\n",
      "episodes 1983\n",
      "exploration 0.038575\n",
      "learning_rate 0.000066\n",
      "Timestep 3740000\n",
      "mean reward (100 episodes) 19.560000\n",
      "best mean reward 19.610000\n",
      "episodes 1989\n",
      "exploration 0.038350\n",
      "learning_rate 0.000066\n",
      "Timestep 3750000\n",
      "mean reward (100 episodes) 19.480000\n",
      "best mean reward 19.610000\n",
      "episodes 1994\n",
      "exploration 0.038125\n",
      "learning_rate 0.000066\n",
      "Timestep 3760000\n",
      "mean reward (100 episodes) 19.420000\n",
      "best mean reward 19.610000\n",
      "episodes 1999\n",
      "exploration 0.037900\n",
      "learning_rate 0.000066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-02 20:13:26,846] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.4.3028.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 3770000\n",
      "mean reward (100 episodes) 19.430000\n",
      "best mean reward 19.610000\n",
      "episodes 2004\n",
      "exploration 0.037675\n",
      "learning_rate 0.000065\n",
      "Timestep 3780000\n",
      "mean reward (100 episodes) 19.500000\n",
      "best mean reward 19.610000\n",
      "episodes 2010\n",
      "exploration 0.037450\n",
      "learning_rate 0.000065\n",
      "Timestep 3790000\n",
      "mean reward (100 episodes) 19.500000\n",
      "best mean reward 19.610000\n",
      "episodes 2015\n",
      "exploration 0.037225\n",
      "learning_rate 0.000065\n",
      "Timestep 3800000\n",
      "mean reward (100 episodes) 19.530000\n",
      "best mean reward 19.610000\n",
      "episodes 2020\n",
      "exploration 0.037000\n",
      "learning_rate 0.000065\n",
      "Timestep 3810000\n",
      "mean reward (100 episodes) 19.530000\n",
      "best mean reward 19.610000\n",
      "episodes 2026\n",
      "exploration 0.036775\n",
      "learning_rate 0.000065\n",
      "Timestep 3820000\n",
      "mean reward (100 episodes) 19.510000\n",
      "best mean reward 19.610000\n",
      "episodes 2031\n",
      "exploration 0.036550\n",
      "learning_rate 0.000065\n",
      "Timestep 3830000\n",
      "mean reward (100 episodes) 19.490000\n",
      "best mean reward 19.610000\n",
      "episodes 2037\n",
      "exploration 0.036325\n",
      "learning_rate 0.000065\n",
      "Timestep 3840000\n",
      "mean reward (100 episodes) 19.440000\n",
      "best mean reward 19.610000\n",
      "episodes 2042\n",
      "exploration 0.036100\n",
      "learning_rate 0.000064\n",
      "Timestep 3850000\n",
      "mean reward (100 episodes) 19.420000\n",
      "best mean reward 19.610000\n",
      "episodes 2048\n",
      "exploration 0.035875\n",
      "learning_rate 0.000064\n",
      "Timestep 3860000\n",
      "mean reward (100 episodes) 19.370000\n",
      "best mean reward 19.610000\n",
      "episodes 2053\n",
      "exploration 0.035650\n",
      "learning_rate 0.000064\n",
      "Timestep 3870000\n",
      "mean reward (100 episodes) 19.370000\n",
      "best mean reward 19.610000\n",
      "episodes 2058\n",
      "exploration 0.035425\n",
      "learning_rate 0.000064\n",
      "Timestep 3880000\n",
      "mean reward (100 episodes) 19.320000\n",
      "best mean reward 19.610000\n",
      "episodes 2063\n",
      "exploration 0.035200\n",
      "learning_rate 0.000064\n",
      "Timestep 3890000\n",
      "mean reward (100 episodes) 19.370000\n",
      "best mean reward 19.610000\n",
      "episodes 2068\n",
      "exploration 0.034975\n",
      "learning_rate 0.000064\n",
      "Timestep 3900000\n",
      "mean reward (100 episodes) 19.370000\n",
      "best mean reward 19.610000\n",
      "episodes 2074\n",
      "exploration 0.034750\n",
      "learning_rate 0.000064\n",
      "Timestep 3910000\n",
      "mean reward (100 episodes) 19.400000\n",
      "best mean reward 19.610000\n",
      "episodes 2079\n",
      "exploration 0.034525\n",
      "learning_rate 0.000064\n",
      "Timestep 3920000\n",
      "mean reward (100 episodes) 19.370000\n",
      "best mean reward 19.610000\n",
      "episodes 2085\n",
      "exploration 0.034300\n",
      "learning_rate 0.000063\n",
      "Timestep 3930000\n",
      "mean reward (100 episodes) 19.410000\n",
      "best mean reward 19.610000\n",
      "episodes 2090\n",
      "exploration 0.034075\n",
      "learning_rate 0.000063\n",
      "Timestep 3940000\n",
      "mean reward (100 episodes) 19.460000\n",
      "best mean reward 19.610000\n",
      "episodes 2096\n",
      "exploration 0.033850\n",
      "learning_rate 0.000063\n",
      "Timestep 3950000\n",
      "mean reward (100 episodes) 19.490000\n",
      "best mean reward 19.610000\n",
      "episodes 2101\n",
      "exploration 0.033625\n",
      "learning_rate 0.000063\n",
      "Timestep 3960000\n",
      "mean reward (100 episodes) 19.450000\n",
      "best mean reward 19.610000\n",
      "episodes 2106\n",
      "exploration 0.033400\n",
      "learning_rate 0.000063\n",
      "Timestep 3970000\n",
      "mean reward (100 episodes) 19.480000\n",
      "best mean reward 19.610000\n",
      "episodes 2112\n",
      "exploration 0.033175\n",
      "learning_rate 0.000063\n",
      "Timestep 3980000\n",
      "mean reward (100 episodes) 19.500000\n",
      "best mean reward 19.610000\n",
      "episodes 2117\n",
      "exploration 0.032950\n",
      "learning_rate 0.000063\n",
      "Timestep 3990000\n",
      "mean reward (100 episodes) 19.470000\n",
      "best mean reward 19.610000\n",
      "episodes 2123\n",
      "exploration 0.032725\n",
      "learning_rate 0.000063\n",
      "Timestep 4000000\n",
      "mean reward (100 episodes) 19.450000\n",
      "best mean reward 19.610000\n",
      "episodes 2128\n",
      "exploration 0.032500\n",
      "learning_rate 0.000063\n",
      "Timestep 4010000\n",
      "mean reward (100 episodes) 19.450000\n",
      "best mean reward 19.610000\n",
      "episodes 2134\n",
      "exploration 0.032275\n",
      "learning_rate 0.000062\n",
      "Timestep 4020000\n",
      "mean reward (100 episodes) 19.500000\n",
      "best mean reward 19.610000\n",
      "episodes 2139\n",
      "exploration 0.032050\n",
      "learning_rate 0.000062\n",
      "Timestep 4030000\n",
      "mean reward (100 episodes) 19.560000\n",
      "best mean reward 19.610000\n",
      "episodes 2145\n",
      "exploration 0.031825\n",
      "learning_rate 0.000062\n",
      "Timestep 4040000\n",
      "mean reward (100 episodes) 19.670000\n",
      "best mean reward 19.670000\n",
      "episodes 2151\n",
      "exploration 0.031600\n",
      "learning_rate 0.000062\n",
      "Timestep 4050000\n",
      "mean reward (100 episodes) 19.670000\n",
      "best mean reward 19.680000\n",
      "episodes 2157\n",
      "exploration 0.031375\n",
      "learning_rate 0.000062\n",
      "Timestep 4060000\n",
      "mean reward (100 episodes) 19.750000\n",
      "best mean reward 19.750000\n",
      "episodes 2162\n",
      "exploration 0.031150\n",
      "learning_rate 0.000062\n",
      "Timestep 4070000\n",
      "mean reward (100 episodes) 19.740000\n",
      "best mean reward 19.750000\n",
      "episodes 2168\n",
      "exploration 0.030925\n",
      "learning_rate 0.000062\n",
      "Timestep 4080000\n",
      "mean reward (100 episodes) 19.760000\n",
      "best mean reward 19.780000\n",
      "episodes 2174\n",
      "exploration 0.030700\n",
      "learning_rate 0.000062\n",
      "Timestep 4090000\n",
      "mean reward (100 episodes) 19.780000\n",
      "best mean reward 19.780000\n",
      "episodes 2180\n",
      "exploration 0.030475\n",
      "learning_rate 0.000061\n",
      "Timestep 4100000\n",
      "mean reward (100 episodes) 19.850000\n",
      "best mean reward 19.850000\n",
      "episodes 2185\n",
      "exploration 0.030250\n",
      "learning_rate 0.000061\n",
      "Timestep 4110000\n",
      "mean reward (100 episodes) 19.840000\n",
      "best mean reward 19.860000\n",
      "episodes 2191\n",
      "exploration 0.030025\n",
      "learning_rate 0.000061\n",
      "Timestep 4120000\n",
      "mean reward (100 episodes) 19.910000\n",
      "best mean reward 19.910000\n",
      "episodes 2197\n",
      "exploration 0.029800\n",
      "learning_rate 0.000061\n",
      "Timestep 4130000\n",
      "mean reward (100 episodes) 19.910000\n",
      "best mean reward 19.920000\n",
      "episodes 2202\n",
      "exploration 0.029575\n",
      "learning_rate 0.000061\n",
      "Timestep 4140000\n",
      "mean reward (100 episodes) 19.980000\n",
      "best mean reward 19.980000\n",
      "episodes 2208\n",
      "exploration 0.029350\n",
      "learning_rate 0.000061\n",
      "Timestep 4150000\n",
      "mean reward (100 episodes) 20.050000\n",
      "best mean reward 20.060000\n",
      "episodes 2214\n",
      "exploration 0.029125\n",
      "learning_rate 0.000061\n",
      "Timestep 4160000\n",
      "mean reward (100 episodes) 20.120000\n",
      "best mean reward 20.120000\n",
      "episodes 2220\n",
      "exploration 0.028900\n",
      "learning_rate 0.000061\n",
      "Timestep 4170000\n",
      "mean reward (100 episodes) 20.150000\n",
      "best mean reward 20.150000\n",
      "episodes 2225\n",
      "exploration 0.028675\n",
      "learning_rate 0.000060\n",
      "Timestep 4180000\n",
      "mean reward (100 episodes) 20.150000\n",
      "best mean reward 20.170000\n",
      "episodes 2231\n",
      "exploration 0.028450\n",
      "learning_rate 0.000060\n",
      "Timestep 4190000\n",
      "mean reward (100 episodes) 20.130000\n",
      "best mean reward 20.170000\n",
      "episodes 2236\n",
      "exploration 0.028225\n",
      "learning_rate 0.000060\n",
      "Timestep 4200000\n",
      "mean reward (100 episodes) 20.110000\n",
      "best mean reward 20.170000\n",
      "episodes 2242\n",
      "exploration 0.028000\n",
      "learning_rate 0.000060\n",
      "Timestep 4210000\n",
      "mean reward (100 episodes) 20.100000\n",
      "best mean reward 20.170000\n",
      "episodes 2247\n",
      "exploration 0.027775\n",
      "learning_rate 0.000060\n",
      "Timestep 4220000\n",
      "mean reward (100 episodes) 20.090000\n",
      "best mean reward 20.170000\n",
      "episodes 2253\n",
      "exploration 0.027550\n",
      "learning_rate 0.000060\n",
      "Timestep 4230000\n",
      "mean reward (100 episodes) 20.070000\n",
      "best mean reward 20.170000\n",
      "episodes 2258\n",
      "exploration 0.027325\n",
      "learning_rate 0.000060\n",
      "Timestep 4240000\n",
      "mean reward (100 episodes) 20.100000\n",
      "best mean reward 20.170000\n",
      "episodes 2264\n",
      "exploration 0.027100\n",
      "learning_rate 0.000060\n",
      "Timestep 4250000\n",
      "mean reward (100 episodes) 20.040000\n",
      "best mean reward 20.170000\n",
      "episodes 2270\n",
      "exploration 0.026875\n",
      "learning_rate 0.000059\n",
      "Timestep 4260000\n",
      "mean reward (100 episodes) 20.030000\n",
      "best mean reward 20.170000\n",
      "episodes 2276\n",
      "exploration 0.026650\n",
      "learning_rate 0.000059\n",
      "Timestep 4270000\n",
      "mean reward (100 episodes) 20.060000\n",
      "best mean reward 20.170000\n",
      "episodes 2282\n",
      "exploration 0.026425\n",
      "learning_rate 0.000059\n",
      "Timestep 4280000\n",
      "mean reward (100 episodes) 20.050000\n",
      "best mean reward 20.170000\n",
      "episodes 2287\n",
      "exploration 0.026200\n",
      "learning_rate 0.000059\n",
      "Timestep 4290000\n",
      "mean reward (100 episodes) 20.040000\n",
      "best mean reward 20.170000\n",
      "episodes 2293\n",
      "exploration 0.025975\n",
      "learning_rate 0.000059\n",
      "Timestep 4300000\n",
      "mean reward (100 episodes) 20.020000\n",
      "best mean reward 20.170000\n",
      "episodes 2299\n",
      "exploration 0.025750\n",
      "learning_rate 0.000059\n",
      "Timestep 4310000\n",
      "mean reward (100 episodes) 20.060000\n",
      "best mean reward 20.170000\n",
      "episodes 2305\n",
      "exploration 0.025525\n",
      "learning_rate 0.000059\n",
      "Timestep 4320000\n",
      "mean reward (100 episodes) 20.070000\n",
      "best mean reward 20.170000\n",
      "episodes 2311\n",
      "exploration 0.025300\n",
      "learning_rate 0.000059\n",
      "Timestep 4330000\n",
      "mean reward (100 episodes) 20.090000\n",
      "best mean reward 20.170000\n",
      "episodes 2317\n",
      "exploration 0.025075\n",
      "learning_rate 0.000058\n",
      "Timestep 4340000\n",
      "mean reward (100 episodes) 20.110000\n",
      "best mean reward 20.170000\n",
      "episodes 2323\n",
      "exploration 0.024850\n",
      "learning_rate 0.000058\n",
      "Timestep 4350000\n",
      "mean reward (100 episodes) 20.130000\n",
      "best mean reward 20.170000\n",
      "episodes 2328\n",
      "exploration 0.024625\n",
      "learning_rate 0.000058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 4360000\n",
      "mean reward (100 episodes) 20.110000\n",
      "best mean reward 20.170000\n",
      "episodes 2334\n",
      "exploration 0.024400\n",
      "learning_rate 0.000058\n",
      "Timestep 4370000\n",
      "mean reward (100 episodes) 20.190000\n",
      "best mean reward 20.190000\n",
      "episodes 2340\n",
      "exploration 0.024175\n",
      "learning_rate 0.000058\n",
      "Timestep 4380000\n",
      "mean reward (100 episodes) 20.240000\n",
      "best mean reward 20.240000\n",
      "episodes 2346\n",
      "exploration 0.023950\n",
      "learning_rate 0.000058\n",
      "Timestep 4390000\n",
      "mean reward (100 episodes) 20.230000\n",
      "best mean reward 20.270000\n",
      "episodes 2352\n",
      "exploration 0.023725\n",
      "learning_rate 0.000058\n",
      "Timestep 4400000\n",
      "mean reward (100 episodes) 20.240000\n",
      "best mean reward 20.270000\n",
      "episodes 2357\n",
      "exploration 0.023500\n",
      "learning_rate 0.000058\n",
      "Timestep 4410000\n",
      "mean reward (100 episodes) 20.230000\n",
      "best mean reward 20.280000\n",
      "episodes 2363\n",
      "exploration 0.023275\n",
      "learning_rate 0.000057\n",
      "Timestep 4420000\n",
      "mean reward (100 episodes) 20.230000\n",
      "best mean reward 20.280000\n",
      "episodes 2369\n",
      "exploration 0.023050\n",
      "learning_rate 0.000057\n",
      "Timestep 4430000\n",
      "mean reward (100 episodes) 20.190000\n",
      "best mean reward 20.280000\n",
      "episodes 2374\n",
      "exploration 0.022825\n",
      "learning_rate 0.000057\n",
      "Timestep 4440000\n",
      "mean reward (100 episodes) 20.130000\n",
      "best mean reward 20.280000\n",
      "episodes 2380\n",
      "exploration 0.022600\n",
      "learning_rate 0.000057\n",
      "Timestep 4450000\n",
      "mean reward (100 episodes) 20.130000\n",
      "best mean reward 20.280000\n",
      "episodes 2386\n",
      "exploration 0.022375\n",
      "learning_rate 0.000057\n",
      "Timestep 4460000\n",
      "mean reward (100 episodes) 20.080000\n",
      "best mean reward 20.280000\n",
      "episodes 2392\n",
      "exploration 0.022150\n",
      "learning_rate 0.000057\n",
      "Timestep 4470000\n",
      "mean reward (100 episodes) 20.120000\n",
      "best mean reward 20.280000\n",
      "episodes 2397\n",
      "exploration 0.021925\n",
      "learning_rate 0.000057\n",
      "Timestep 4480000\n",
      "mean reward (100 episodes) 20.110000\n",
      "best mean reward 20.280000\n",
      "episodes 2403\n",
      "exploration 0.021700\n",
      "learning_rate 0.000057\n",
      "Timestep 4490000\n",
      "mean reward (100 episodes) 20.050000\n",
      "best mean reward 20.280000\n",
      "episodes 2409\n",
      "exploration 0.021475\n",
      "learning_rate 0.000056\n",
      "Timestep 4500000\n",
      "mean reward (100 episodes) 19.950000\n",
      "best mean reward 20.280000\n",
      "episodes 2414\n",
      "exploration 0.021250\n",
      "learning_rate 0.000056\n",
      "Timestep 4510000\n",
      "mean reward (100 episodes) 19.850000\n",
      "best mean reward 20.280000\n",
      "episodes 2419\n",
      "exploration 0.021025\n",
      "learning_rate 0.000056\n",
      "Timestep 4520000\n",
      "mean reward (100 episodes) 19.850000\n",
      "best mean reward 20.280000\n",
      "episodes 2424\n",
      "exploration 0.020800\n",
      "learning_rate 0.000056\n",
      "Timestep 4530000\n",
      "mean reward (100 episodes) 19.880000\n",
      "best mean reward 20.280000\n",
      "episodes 2430\n",
      "exploration 0.020575\n",
      "learning_rate 0.000056\n",
      "Timestep 4540000\n",
      "mean reward (100 episodes) 19.830000\n",
      "best mean reward 20.280000\n",
      "episodes 2436\n",
      "exploration 0.020350\n",
      "learning_rate 0.000056\n",
      "Timestep 4550000\n",
      "mean reward (100 episodes) 19.790000\n",
      "best mean reward 20.280000\n",
      "episodes 2442\n",
      "exploration 0.020125\n",
      "learning_rate 0.000056\n",
      "Timestep 4560000\n",
      "mean reward (100 episodes) 19.780000\n",
      "best mean reward 20.280000\n",
      "episodes 2448\n",
      "exploration 0.019900\n",
      "learning_rate 0.000056\n",
      "Timestep 4570000\n",
      "mean reward (100 episodes) 19.760000\n",
      "best mean reward 20.280000\n",
      "episodes 2453\n",
      "exploration 0.019675\n",
      "learning_rate 0.000055\n",
      "Timestep 4580000\n",
      "mean reward (100 episodes) 19.780000\n",
      "best mean reward 20.280000\n",
      "episodes 2459\n",
      "exploration 0.019450\n",
      "learning_rate 0.000055\n",
      "Timestep 4590000\n",
      "mean reward (100 episodes) 19.820000\n",
      "best mean reward 20.280000\n",
      "episodes 2465\n",
      "exploration 0.019225\n",
      "learning_rate 0.000055\n",
      "Timestep 4600000\n",
      "mean reward (100 episodes) 19.840000\n",
      "best mean reward 20.280000\n",
      "episodes 2471\n",
      "exploration 0.019000\n",
      "learning_rate 0.000055\n",
      "Timestep 4610000\n",
      "mean reward (100 episodes) 19.880000\n",
      "best mean reward 20.280000\n",
      "episodes 2477\n",
      "exploration 0.018775\n",
      "learning_rate 0.000055\n",
      "Timestep 4620000\n",
      "mean reward (100 episodes) 19.960000\n",
      "best mean reward 20.280000\n",
      "episodes 2483\n",
      "exploration 0.018550\n",
      "learning_rate 0.000055\n",
      "Timestep 4630000\n",
      "mean reward (100 episodes) 19.960000\n",
      "best mean reward 20.280000\n",
      "episodes 2489\n",
      "exploration 0.018325\n",
      "learning_rate 0.000055\n",
      "Timestep 4640000\n",
      "mean reward (100 episodes) 19.980000\n",
      "best mean reward 20.280000\n",
      "episodes 2495\n",
      "exploration 0.018100\n",
      "learning_rate 0.000055\n",
      "Timestep 4650000\n",
      "mean reward (100 episodes) 19.980000\n",
      "best mean reward 20.280000\n",
      "episodes 2500\n",
      "exploration 0.017875\n",
      "learning_rate 0.000054\n",
      "Timestep 4660000\n",
      "mean reward (100 episodes) 19.950000\n",
      "best mean reward 20.280000\n",
      "episodes 2506\n",
      "exploration 0.017650\n",
      "learning_rate 0.000054\n",
      "Timestep 4670000\n",
      "mean reward (100 episodes) 20.080000\n",
      "best mean reward 20.280000\n",
      "episodes 2512\n",
      "exploration 0.017425\n",
      "learning_rate 0.000054\n",
      "Timestep 4680000\n",
      "mean reward (100 episodes) 20.130000\n",
      "best mean reward 20.280000\n",
      "episodes 2518\n",
      "exploration 0.017200\n",
      "learning_rate 0.000054\n",
      "Timestep 4690000\n",
      "mean reward (100 episodes) 20.170000\n",
      "best mean reward 20.280000\n",
      "episodes 2524\n",
      "exploration 0.016975\n",
      "learning_rate 0.000054\n",
      "Timestep 4700000\n",
      "mean reward (100 episodes) 20.190000\n",
      "best mean reward 20.280000\n",
      "episodes 2530\n",
      "exploration 0.016750\n",
      "learning_rate 0.000054\n",
      "Timestep 4710000\n",
      "mean reward (100 episodes) 20.260000\n",
      "best mean reward 20.280000\n",
      "episodes 2536\n",
      "exploration 0.016525\n",
      "learning_rate 0.000054\n",
      "Timestep 4720000\n",
      "mean reward (100 episodes) 20.310000\n",
      "best mean reward 20.310000\n",
      "episodes 2542\n",
      "exploration 0.016300\n",
      "learning_rate 0.000053\n",
      "Timestep 4730000\n",
      "mean reward (100 episodes) 20.300000\n",
      "best mean reward 20.320000\n",
      "episodes 2548\n",
      "exploration 0.016075\n",
      "learning_rate 0.000053\n",
      "Timestep 4740000\n",
      "mean reward (100 episodes) 20.380000\n",
      "best mean reward 20.380000\n",
      "episodes 2554\n",
      "exploration 0.015850\n",
      "learning_rate 0.000053\n",
      "Timestep 4750000\n",
      "mean reward (100 episodes) 20.420000\n",
      "best mean reward 20.420000\n",
      "episodes 2561\n",
      "exploration 0.015625\n",
      "learning_rate 0.000053\n",
      "Timestep 4760000\n",
      "mean reward (100 episodes) 20.390000\n",
      "best mean reward 20.420000\n",
      "episodes 2566\n",
      "exploration 0.015400\n",
      "learning_rate 0.000053\n",
      "Timestep 4770000\n",
      "mean reward (100 episodes) 20.420000\n",
      "best mean reward 20.420000\n",
      "episodes 2572\n",
      "exploration 0.015175\n",
      "learning_rate 0.000053\n",
      "Timestep 4780000\n",
      "mean reward (100 episodes) 20.420000\n",
      "best mean reward 20.430000\n",
      "episodes 2578\n",
      "exploration 0.014950\n",
      "learning_rate 0.000053\n",
      "Timestep 4790000\n",
      "mean reward (100 episodes) 20.370000\n",
      "best mean reward 20.430000\n",
      "episodes 2584\n",
      "exploration 0.014725\n",
      "learning_rate 0.000053\n",
      "Timestep 4800000\n",
      "mean reward (100 episodes) 20.360000\n",
      "best mean reward 20.430000\n",
      "episodes 2590\n",
      "exploration 0.014500\n",
      "learning_rate 0.000053\n",
      "Timestep 4810000\n",
      "mean reward (100 episodes) 20.360000\n",
      "best mean reward 20.430000\n",
      "episodes 2596\n",
      "exploration 0.014275\n",
      "learning_rate 0.000052\n",
      "Timestep 4820000\n",
      "mean reward (100 episodes) 20.360000\n",
      "best mean reward 20.430000\n",
      "episodes 2602\n",
      "exploration 0.014050\n",
      "learning_rate 0.000052\n",
      "Timestep 4830000\n",
      "mean reward (100 episodes) 20.390000\n",
      "best mean reward 20.430000\n",
      "episodes 2608\n",
      "exploration 0.013825\n",
      "learning_rate 0.000052\n",
      "Timestep 4840000\n",
      "mean reward (100 episodes) 20.400000\n",
      "best mean reward 20.430000\n",
      "episodes 2615\n",
      "exploration 0.013600\n",
      "learning_rate 0.000052\n",
      "Timestep 4850000\n",
      "mean reward (100 episodes) 20.430000\n",
      "best mean reward 20.450000\n",
      "episodes 2621\n",
      "exploration 0.013375\n",
      "learning_rate 0.000052\n",
      "Timestep 4860000\n",
      "mean reward (100 episodes) 20.410000\n",
      "best mean reward 20.450000\n",
      "episodes 2627\n",
      "exploration 0.013150\n",
      "learning_rate 0.000052\n",
      "Timestep 4870000\n",
      "mean reward (100 episodes) 20.360000\n",
      "best mean reward 20.450000\n",
      "episodes 2631\n",
      "exploration 0.012925\n",
      "learning_rate 0.000052\n",
      "Timestep 4880000\n",
      "mean reward (100 episodes) 20.330000\n",
      "best mean reward 20.450000\n",
      "episodes 2637\n",
      "exploration 0.012700\n",
      "learning_rate 0.000052\n",
      "Timestep 4890000\n",
      "mean reward (100 episodes) 20.140000\n",
      "best mean reward 20.450000\n",
      "episodes 2642\n",
      "exploration 0.012475\n",
      "learning_rate 0.000051\n",
      "Timestep 4900000\n",
      "mean reward (100 episodes) 20.140000\n",
      "best mean reward 20.450000\n",
      "episodes 2648\n",
      "exploration 0.012250\n",
      "learning_rate 0.000051\n",
      "Timestep 4910000\n",
      "mean reward (100 episodes) 20.150000\n",
      "best mean reward 20.450000\n",
      "episodes 2654\n",
      "exploration 0.012025\n",
      "learning_rate 0.000051\n",
      "Timestep 4920000\n",
      "mean reward (100 episodes) 20.180000\n",
      "best mean reward 20.450000\n",
      "episodes 2660\n",
      "exploration 0.011800\n",
      "learning_rate 0.000051\n",
      "Timestep 4930000\n",
      "mean reward (100 episodes) 20.210000\n",
      "best mean reward 20.450000\n",
      "episodes 2666\n",
      "exploration 0.011575\n",
      "learning_rate 0.000051\n",
      "Timestep 4940000\n",
      "mean reward (100 episodes) 20.220000\n",
      "best mean reward 20.450000\n",
      "episodes 2672\n",
      "exploration 0.011350\n",
      "learning_rate 0.000051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 4950000\n",
      "mean reward (100 episodes) 20.210000\n",
      "best mean reward 20.450000\n",
      "episodes 2678\n",
      "exploration 0.011125\n",
      "learning_rate 0.000051\n",
      "Timestep 4960000\n",
      "mean reward (100 episodes) 20.250000\n",
      "best mean reward 20.450000\n",
      "episodes 2685\n",
      "exploration 0.010900\n",
      "learning_rate 0.000051\n",
      "Timestep 4970000\n",
      "mean reward (100 episodes) 20.270000\n",
      "best mean reward 20.450000\n",
      "episodes 2691\n",
      "exploration 0.010675\n",
      "learning_rate 0.000050\n",
      "Timestep 4980000\n",
      "mean reward (100 episodes) 20.300000\n",
      "best mean reward 20.450000\n",
      "episodes 2697\n",
      "exploration 0.010450\n",
      "learning_rate 0.000050\n",
      "Timestep 4990000\n",
      "mean reward (100 episodes) 20.380000\n",
      "best mean reward 20.450000\n",
      "episodes 2703\n",
      "exploration 0.010225\n",
      "learning_rate 0.000050\n",
      "Timestep 5000000\n",
      "mean reward (100 episodes) 20.340000\n",
      "best mean reward 20.450000\n",
      "episodes 2709\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5010000\n",
      "mean reward (100 episodes) 20.320000\n",
      "best mean reward 20.450000\n",
      "episodes 2715\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5020000\n",
      "mean reward (100 episodes) 20.320000\n",
      "best mean reward 20.450000\n",
      "episodes 2721\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5030000\n",
      "mean reward (100 episodes) 20.410000\n",
      "best mean reward 20.450000\n",
      "episodes 2728\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5040000\n",
      "mean reward (100 episodes) 20.500000\n",
      "best mean reward 20.500000\n",
      "episodes 2734\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5050000\n",
      "mean reward (100 episodes) 20.600000\n",
      "best mean reward 20.600000\n",
      "episodes 2740\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5060000\n",
      "mean reward (100 episodes) 20.690000\n",
      "best mean reward 20.700000\n",
      "episodes 2746\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5070000\n",
      "mean reward (100 episodes) 20.690000\n",
      "best mean reward 20.710000\n",
      "episodes 2752\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5080000\n",
      "mean reward (100 episodes) 20.640000\n",
      "best mean reward 20.710000\n",
      "episodes 2758\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5090000\n",
      "mean reward (100 episodes) 20.550000\n",
      "best mean reward 20.710000\n",
      "episodes 2764\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5100000\n",
      "mean reward (100 episodes) 20.510000\n",
      "best mean reward 20.710000\n",
      "episodes 2770\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5110000\n",
      "mean reward (100 episodes) 20.500000\n",
      "best mean reward 20.710000\n",
      "episodes 2776\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5120000\n",
      "mean reward (100 episodes) 20.490000\n",
      "best mean reward 20.710000\n",
      "episodes 2782\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5130000\n",
      "mean reward (100 episodes) 20.490000\n",
      "best mean reward 20.710000\n",
      "episodes 2788\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5140000\n",
      "mean reward (100 episodes) 20.480000\n",
      "best mean reward 20.710000\n",
      "episodes 2794\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5150000\n",
      "mean reward (100 episodes) 20.470000\n",
      "best mean reward 20.710000\n",
      "episodes 2800\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5160000\n",
      "mean reward (100 episodes) 20.460000\n",
      "best mean reward 20.710000\n",
      "episodes 2806\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5170000\n",
      "mean reward (100 episodes) 20.490000\n",
      "best mean reward 20.710000\n",
      "episodes 2812\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5180000\n",
      "mean reward (100 episodes) 20.460000\n",
      "best mean reward 20.710000\n",
      "episodes 2818\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5190000\n",
      "mean reward (100 episodes) 20.390000\n",
      "best mean reward 20.710000\n",
      "episodes 2824\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5200000\n",
      "mean reward (100 episodes) 20.390000\n",
      "best mean reward 20.710000\n",
      "episodes 2830\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5210000\n",
      "mean reward (100 episodes) 20.390000\n",
      "best mean reward 20.710000\n",
      "episodes 2836\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5220000\n",
      "mean reward (100 episodes) 20.360000\n",
      "best mean reward 20.710000\n",
      "episodes 2842\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5230000\n",
      "mean reward (100 episodes) 20.380000\n",
      "best mean reward 20.710000\n",
      "episodes 2849\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5240000\n",
      "mean reward (100 episodes) 20.410000\n",
      "best mean reward 20.710000\n",
      "episodes 2855\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5250000\n",
      "mean reward (100 episodes) 20.450000\n",
      "best mean reward 20.710000\n",
      "episodes 2861\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5260000\n",
      "mean reward (100 episodes) 20.480000\n",
      "best mean reward 20.710000\n",
      "episodes 2867\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5270000\n",
      "mean reward (100 episodes) 20.510000\n",
      "best mean reward 20.710000\n",
      "episodes 2874\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5280000\n",
      "mean reward (100 episodes) 20.560000\n",
      "best mean reward 20.710000\n",
      "episodes 2880\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5290000\n",
      "mean reward (100 episodes) 20.560000\n",
      "best mean reward 20.710000\n",
      "episodes 2886\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5300000\n",
      "mean reward (100 episodes) 20.590000\n",
      "best mean reward 20.710000\n",
      "episodes 2892\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5310000\n",
      "mean reward (100 episodes) 20.580000\n",
      "best mean reward 20.710000\n",
      "episodes 2898\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5320000\n",
      "mean reward (100 episodes) 20.570000\n",
      "best mean reward 20.710000\n",
      "episodes 2904\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5330000\n",
      "mean reward (100 episodes) 20.580000\n",
      "best mean reward 20.710000\n",
      "episodes 2910\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "Timestep 5340000\n",
      "mean reward (100 episodes) 20.620000\n",
      "best mean reward 20.710000\n",
      "episodes 2916\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-079aeb735d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_Atari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-14116a62d0cc>\u001b[0m in \u001b[0;36mmain_Atari\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0matari_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6b51f7da7699>\u001b[0m in \u001b[0;36matari_learn\u001b[0;34m(env, session, num_timesteps)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mframe_history_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtarget_update_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mgrad_norm_clipping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/projects/CS294/MyHomework/hw3/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(env, q_func, optimizer_spec, session, exploration, stopping_criterion, replay_buffer_size, batch_size, gamma, learning_starts, learning_freq, frame_history_len, target_update_freq, grad_norm_clipping)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/projects/CS294/MyHomework/hw3/dqn_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_n_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_in_buffer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_recent_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/projects/CS294/MyHomework/hw3/dqn_utils.py\u001b[0m in \u001b[0;36m_encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mobs_batch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mact_batch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mrew_batch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_Atari()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
